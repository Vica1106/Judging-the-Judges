{"Major": "Statistics", "Term": "mutual independence", "Explanation": "- Definition: Mutual independence means a group of events has no influence on each other, so the probability of any combination of them happening is simply the product of their individual probabilities.\n\n- Real-life analogy: Imagine flipping three fair coins at the same time; the result of one coin doesn't affect the others, so the chance all three land heads is (1/2)^3.\n\n- Concrete example: Example: three fair coins; A = first is heads, B = second is heads, C = third is heads. Then P(A ∩ B ∩ C) = 1/8 and P(A)P(B)P(C) = 1/8, and the same holds for any subset.\n\n- Takeaway: If events are mutual independence, you can multiply their chances to get a combined probability; if any dependence exists, that simple multiplying rule breaks down and you need more careful calculation."}
{"Major": "Statistics", "Term": "statistical inference", "Explanation": "- Definition: Using data from a small group to draw conclusions about a larger group, a process called statistical inference, while keeping in mind that the conclusion may be wrong.\n\n- Real-life analogy: Like tasting a spoonful of soup to guess how the whole pot will taste.\n\n- Concrete example: You survey 200 students about how many hours they sleep per night, then compute the average. This gives an estimate of the typical sleep for all students at your college, with some uncertainty.\n\n- Takeaway: It lets you learn big questions from a sample, and you get a sense of how confident or uncertain your answer is."}
{"Major": "Statistics", "Term": "joint distribution", "Explanation": "- Definition: Joint distribution describes how two or more variables occur together, by listing the probability for every possible combination of their values.\n\n- Real-life analogy: Think of a weather forecast that shows the chance of every combo of temperature (cold/mild/hot) and rain (yes/no) happening together.\n\n- Concrete example: Suppose temperature has two categories (Cold, Warm) and rain has two (Yes, No). The joint distribution could be: Cold+Rain 0.1, Cold+No Rain 0.4, Warm+Rain 0.2, Warm+No Rain 0.3.\n\n- Takeaway: It’s a compact map of how two things relate, so you can predict or reason about any combined outcome and see whether the variables tend to go together. If you know one variable, you can update your guess about the other."}
{"Major": "Statistics", "Term": "random variable", "Explanation": "- Definition: A random variable is a rule that assigns a number to the outcome of a random process, so we can study it with math.  \n- Real-life analogy: Like a slot machine that shows a different number each time you pull the lever—the number you see depends on luck.  \n- Concrete example: If you roll a standard six-sided die and set X to be the number that comes up, X can be 1, 2, 3, 4, 5, or 6, each with equal chance.  \n- Takeaway: It turns unpredictable events into numbers you can analyze, helping you estimate averages, probabilities, and patterns in practice."}
{"Major": "Statistics", "Term": "confidence interval (CI)", "Explanation": "- Definition: A confidence interval (CI) is a range of numbers based on your sample data that is likely to contain the true value you’re trying to learn.\n\n- Real-life analogy: It’s like a weather forecast — after looking at readings, you say the true temperature is probably between 68 and 72 degrees.\n\n- Concrete example: You survey 30 students, estimate the average score and report a 95% CI of 72 to 78. That means you’re 95% confident the true average lies in that range; if you did many such studies, about 95% of the intervals would contain the real average.\n\n- Takeaway: A CI shows the uncertainty in your estimate by giving a plausible range, not just a single number. A narrower CI is more precise; a wider CI is more confident about capturing the truth but less precise. Quick summary: CI = plausible range for the true value based on data."}
{"Major": "Statistics", "Term": "covariance", "Explanation": "- Definition: Covariance is a measure of how two things change together; when they tend to rise together, it's positive, and when one rises while the other falls, it's negative.\n\n- Real-life analogy: Imagine two dancers who tend to move in sync; when one steps forward, the other tends to step forward too.\n\n- Concrete example: Hours studied and exam scores for a class; more study hours usually go with higher scores, so the covariance is positive.\n\n- Takeaway: Covariance helps you see if two things move in the same direction and roughly how closely they do, with positive meaning together and negative meaning opposite."}
{"Major": "Statistics", "Term": "likelihood function", "Explanation": "- Definition: The likelihood function is a way to score how likely our data are given different values of the thing we're estimating.\n\n- Real-life analogy: Think of being a detective with clues—for each possible suspect you rate how well the clues fit; the higher the rating, the more likely that suspect.\n\n- Concrete example: You flip a coin 10 times and see 7 heads. For every bias p between 0 and 1, the likelihood function tells you how probable that 7-head outcome is under that p; it’s highest near the p that best explains the data.\n\n- Takeaway: Likelihood helps you compare different guesses about the world using the actual data you observed, pointing you toward the best-fitting value."}
{"Major": "Statistics", "Term": "probability measure", "Explanation": "- Definition: A probability measure is a rule that assigns a number from 0 to 1 to every possible event (a set of outcomes) in a consistent way, with the whole space totaling 1.\n\n- Real-life analogy: imagine a bag of colored balls; the probability measure is how likely you are to draw each color, and all the chances add up to 1.\n\n- Concrete example: Rolling a fair six-sided die gives each face a probability of 1/6, and the probabilities of all six faces add up to 1.\n\n- Takeaway: probability measures give a clean, reliable way to quantify and compare likelihoods, helping you reason about chances and uncertainty."}
{"Major": "Statistics", "Term": "regression analysis", "Explanation": "- Definition: regression analysis is a method for understanding how one thing changes as another thing changes.\n\n- Real-life analogy: it's like drawing the best-fit path on a map that shows how study hours relate to test scores—the line shows the usual outcome for a given study time.\n\n- Concrete example: 2h → 70, 4h → 78, 6h → 85; regression gives the predicted score for 5h and shows how scores typically vary around it.\n\n- Takeaway: it helps you predict outcomes from inputs and judge how strong the relationship is, using a simple line you can rely on."}
{"Major": "Statistics", "Term": "causal study", "Explanation": "- Definition: causal study is an investigation that tries to determine whether one thing causes another.\n- Real-life analogy: It’s like testing a new study plan to see if it makes grades better, by giving one group the plan and keeping the rest of the class the same.\n- Concrete example: A teacher splits two classes with similar abilities; for a month, one class uses a new hands-on method while the other sticks to the usual method, then compares test scores.\n- Takeaway: It matters because comparing similar groups that differ only in the factor you test helps show genuine cause-and-effect, not just a coincidence."}
