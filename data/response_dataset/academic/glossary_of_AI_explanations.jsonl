{"Major": "Artificial Intelligence", "Term": "Selective Linear Definite Clause Resolution", "Explanation": "Selective Linear Definite Clause Resolution (SLD resolution) is the procedural inference mechanism for definite logic programs. A definite clause has the form H :- B1, ..., Bn (n ≥ 0); facts are clauses with n = 0. A query (goal) is a finite sequence A1, ..., Am of atoms.\n\nDerivation step. Let G be the current goal and select an atom Ai according to a selection rule. Let P contain a clause H :- B1, ..., Bn and let theta be the most general unifier (mgu) of Ai and H. If unifiable, form the resolvent G' = (A1 theta, ..., A(i−1) theta, B1 theta, ..., Bn theta, A(i+1) theta, ..., Am theta). Replace Ai by the body of the clause and apply theta to the entire goal.\n\nA derivation is a (finite or infinite) sequence of steps. A refutation is a derivation that yields the empty goal. The SLD-tree represents all possible derivations from a given query. Properties: soundness (computed answers are logical consequences of the program) and, under a complete search strategy, completeness with respect to the least Herbrand model of the program. Termination is not guaranteed in general."}
{"Major": "Artificial Intelligence", "Term": "Big O notation", "Explanation": "Definition and purpose\n- Let f, g: N → R_+ denote resource usage (e.g., time, space) as a function of input size n.\n- f(n) = O(g(n)) if ∃ constants C > 0 and n0 ∈ N such that ∀ n ≥ n0, f(n) ≤ C g(n).\n\nRelated notations\n- f(n) = Ω(g(n)) if ∃ C > 0 and n0 with ∀ n ≥ n0, f(n) ≥ C g(n).\n- f(n) ∈ Θ(g(n)) if f(n) = O(g(n)) and f(n) = Ω(g(n)).\n- f(n) = o(g(n)) if lim_{n→∞} f(n)/g(n) = 0.\n\nInterpretation\n- These definitions capture asymptotic growth, suppressing constant factors and lower-order terms.\n- They classify algorithmic complexity (time, space) by dominant terms for large n.\n\nCommon growth classes\n- Polynomial: n^k\n- Logarithmic: log n\n- Exponential: a^n\n- Sub-/super-polynomial distinctions derive from the above.\n\nExample\n- f(n) = 3n^2 + 2n, g(n) = n^2. Then f ∈ O(g) (e.g., C = 5, n0 = 1).\n\nAI relevance\n- Big O enables formal scalability comparisons for search, optimization, and learning procedures."}
{"Major": "Artificial Intelligence", "Term": "neural machine translation (NMT)", "Explanation": "Neural machine translation (NMT) is the task of learning a parametric conditional distribution pθ(y|x) that assigns high probability to correct translations of a source sequence x in a target language. Let X = (x1,…,xT) ∈ Vx^T and Y = (y1,…,yU) ∈ Vy^U. Given a parallel corpus D = { (x(i), y(i)) }, NMT optimizes θ to maximize the log-likelihood ∑i log pθ(y(i)|x(i)). Under an autoregressive sequence model, pθ(y|x) factorizes as ∏t pθ(yt | y1,…,yt−1, x). The conditional distributions are realized by a neural encoder–decoder: the encoder fenc maps x to a representation h, and the decoder fdec generates yt conditioned on h and previously generated tokens; attention weights a allows c_t = a(h, y<sub>t</sub>) to influence yt. In practice, architectures such as the Transformer (multi-head self-attention, positional encodings) are prevalent. Inference uses autoregressive decoding (greedy or beam search) to approximate argmaxy pθ(y|x). Training employs cross-entropy loss with teacher forcing and stochastic optimization over large parallel corpora, possibly with subword tokenization (e.g., BPE). Evaluation typically uses BLEU. Advantages include end-to-end optimization and strong modeling of global dependencies; challenges include data requirements and domain transfer."}
{"Major": "Artificial Intelligence", "Term": "NP-hardness", "Explanation": "Definition. Let A ⊆ Σ* be a decision problem (a language). A is NP-hard if, for every L ∈ NP, there exists a polynomial-time computable function f: Σ* → Σ* such that ∀x, x ∈ L ⇔ f(x) ∈ A. Equivalently, L ≤p,m A (polynomial-time many-one reduction). Thus A is at least as hard as any problem in NP.\n\nRemarks.\n- If A ∈ NP and A is NP-hard, then A is NP-complete.\n- NP-hardness is a property of decision problems; optimization problems are NP-hard when their associated decision problem (e.g., whether a solution of value ≥ k exists) is NP-hard.\n- Reductions used are typically polynomial-time many-one reductions; their transitivity implies that solving A in polynomial time would yield polynomial-time solutions for all NP problems, under P ≠ NP assumptions.\n\nConsequences. NP-hardness provides a formal measure of intractability: unless P = NP, no polynomial-time algorithm exists for NP-hard problems."}
{"Major": "Artificial Intelligence", "Term": "true quantified Boolean formula", "Explanation": "A true quantified Boolean formula (QBF) is a closed, quantified propositional formula that evaluates to true under standard alternating-quantifier semantics.\n\nFormal: Let Φ = Q1 x1 Q2 x2 ... Qn xn φ, where each Qi ∈ {∃, ∀} and φ is a propositional formula over variables x1,...,xn. Define a truth function V_k: {0,1}^{k-1} → {0,1} by\n- V_{n+1}(a1,...,an) = Val_φ(a1,...,an), the truth value of φ under the assignment a1,...,an.\n- For k ≤ n, V_k(a1,...,a_{k-1}) =\n  - max{ V_{k+1}(a1,...,a_k) : ak ∈ {0,1} } if Qk = ∃,\n  - min{ V_{k+1}(a1,...,a_k) : ak ∈ {0,1} } if Qk = ∀.\n\nThe QBF Φ is true (valid) iff V_1(∅) = 1. Equivalently, the closed formula is true under every permissible interpretation of the quantified variables. The problem of deciding truth for QBF is PSPACE-complete."}
{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "- Definition: Algorithmic probability, or Solomonoff probability, assigns to each finite string x a priori probability\n  m(x) = ∑_{p: U(p)=x} 2^{-|p|},\n  where U is a fixed universal prefix-free Turing machine and the sum ranges over all halting programs p that output x. Prefix-freeness ensures ∑_x m(x) ≤ 1.\n\n- Relation to Kolmogorov complexity: Let K(x) = min{|p| : U(p)=x}. Then m(x) is asymptotically inverse to complexity: K(x) ≤ −log m(x) + O(1) and m(x) ≤ O(2^{−K(x)}).\n\n- Computability: m is incomputable; it is enumerable from below (semicomputable) but cannot be computed exactly by any algorithm.\n\n- Interpretation and use: m serves as a universal prior over strings. In Solomonoff induction, the predictive distribution for data conditioned on a hypothesis is m(x|y) = m(x,y)/m(y), providing a formal framework for universal prediction based on all computable hypotheses weighted by their simplicity."}
{"Major": "Artificial Intelligence", "Term": "behavior informatics (BI)", "Explanation": "Definition and scope\n\nBehavior informatics (BI) is an interdisciplinary science that studies the acquisition, representation, modeling, analysis, and management of behavior information—the data encoding observable and latent behavioral states of agents (humans, autonomous systems, or hybrids)—to enable prediction, explanation, and control within information-intensive environments.\n\nCore constructs\n\n- Behavior information: formal representations (ontologies, taxonomies, schemas) of behavioral states, events, and transitions.\n- Behavioral modeling: formal models of dynamics (statistical, probabilistic, temporal, agent-based, or hybrid) capturing how behaviors evolve over time.\n- Behavioral analytics: methods for discovery, inference, prediction, explanation, and decision support.\n- Behavioral systems design: integration of BI into information systems to enhance usability, adaptivity, and governance.\n\nRelation to AI\n\nBI supplies structured behavioral knowledge to AI systems and exploits machine learning, data mining, reasoning, planning, and multi-agent techniques. Emphasis is placed on semantic interoperability, explainability, and domain transferability.\n\nMethods and objectives\n\nData collection and preprocessing; sequence mining; probabilistic graphical models; temporal logic; rule-based and ontology-based reasoning; reinforcement learning. Objectives include formalization of behavior, reproducible analyses, automated decision support, and cross-domain interoperability.\n\nChallenges\n\nData heterogeneity and privacy, causal inference versus correlation, interpretability, scalability, and evaluation benchmarks."}
{"Major": "Artificial Intelligence", "Term": "big data", "Explanation": "Big data is a term for data sets whose scale, generation rate, and heterogeneity exceed the capabilities of traditional data-processing technologies to capture, store, manage, and analyze within required latency.\n\nCore characteristics (the canonical \"3Vs\" plus extensions):\n- Volume: extremely large data corpora (from terabytes to exabytes) that overwhelm single-machine storage.\n- Velocity: data arrive as continuous streams or at high throughput, demanding incremental or near-real-time processing.\n- Variety: data originate from diverse sources and exist in multiple models (structured, semi-structured, unstructured), necessitating flexible schemas and integration.\n\nAdditional attributes often discussed:\n- Veracity: uncertainty and quality concerns in data.\n- Value: extraction of meaningful, actionable insights.\n- Variability: fluctuations in data flow and meaning over time.\n- Complexity: intricate interdependencies among data elements.\n\nArchitectural and analytical implications:\n- Requires distributed storage and parallel computation (e.g., distributed file systems, MapReduce/Spark).\n- Supports schema-on-read and robust data governance.\n- Analytics aim to derive scalable, actionable insights using specialized algorithms and data-management techniques beyond traditional relational paradigms."}
{"Major": "Artificial Intelligence", "Term": "convolutional neural network", "Explanation": "A convolutional neural network (CNN) is a parametric function composed of stacked layers that operate on grid-structured data (e.g., images) using local, weight-shared, translation-equivariant linear mappings. The core unit is the convolutional layer: for input X ∈ R^{H×W×C_in}, a set of K learnable kernels W_k ∈ R^{h×w×C_in} with biases b_k ∈ R is applied to produce feature maps Z_k via\n\nZ_k(i,j) = φ( ∑_{c=1}^{C_in} ∑_{u=−p}^{p} ∑_{v=−q}^{q} W_k,c,u,v X_c(i+u, j+v) + b_k )\n\nwhere φ is a nonlinear activation and padding p, q, stride s determine the output spatial dimensions H', W' (H' = floor((H + 2P_h − h)/S) + 1, similarly for W). Convolution is typically followed by pooling (e.g., max/average) for downsampling, and normalization layers may be interleaved. The network maps X to an output via a composition f_θ(X) with θ collecting all weights and biases. Training proceeds end-to-end by backpropagation minimizing a supervised loss over a dataset. Key properties include local connectivity, parameter sharing, and translation equivariance, enabling hierarchical, spatially invariant representations."}
{"Major": "Artificial Intelligence", "Term": "Darkforest", "Explanation": "Darkforest (DF) is an AI-adjacent term borrowed from the Dark Forest hypothesis in Liu Cixin’s fiction, used as a formal metaphor for strategic concealment in multi‑agent systems. It denotes environments where inter-agent visibility raises existential risk (e.g., retaliation, exploitation), incentivizing agents to minimize detectability rather than maximize traditional coordination.\n\nFormal conception. Let G = (N, A_i, S_i, O_i, u_i, p_i) be a finite, incomplete-information multi-agent game, where:\n- i ∈ N, A_i is the action set, S_i ⊆ {0,1} is a signaling variable (1 = reveal/emit detectable signals),\n- O_i is the observation set, and u_i(a, s) is the expected material payoff given action profile a and signal profile s,\n- p_i(s_i, s_-i) is the exposure cost or risk induced by detectable signaling.\n\nA Darkforest equilibrium is a strategy profile (a_i^*, s_i^*) such that, for all i, given beliefs about others, s_i^* = 0 (silent) is optimal, because the marginal expected payoff from signaling (increased coordination benefits minus increased exposure risk) is negative. The DF regime thus yields low detectability, potentially at the expense of coordination benefits.\n\nImplications for AI: reinforces the design of privacy-preserving, robustly safe multi-agent protocols and cryptographic coordination to mitigate risks of disclosure."}
