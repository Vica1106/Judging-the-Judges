{"Major": "Statistics", "Term": "mutual independence", "Explanation": "Mutual independence (of random variables)\n\nLet (Ω, F, P) be a probability space and X_1, …, X_n be random variables with respective Borel σ-algebras. The collection {X_1, …, X_n} is mutually independent if, for every finite subset J ⊆ {1,…,n} and for all B_j ∈ Σ_j (j ∈ J),\nP(⋂_{j∈J} {X_j ∈ B_j}) = ∏_{j∈J} P(X_j ∈ B_j).\n\nEquivalently, the σ-algebras σ(X_i : i ∈ {1,…,n}) are independent: for all finite J and all B_j ∈ Σ_j,\nP(⋂_{j∈J} {X_j ∈ B_j}) = ∏_{j∈J} P(X_j ∈ B_j).\n\nFor discrete-valued variables, mutual independence is equivalent to\nP(X_1 = x_1, …, X_n = x_n) = ∏_{i=1}^n P(X_i = x_i)\nfor all tuples (x_1, …, x_n).\n\nRelation to other concepts: mutual independence implies pairwise independence, but pairwise independence does not in general imply mutual independence.\n\nConsequences: for any bounded measurable functions f_i, E[∏_{i=1}^n f_i(X_i)] = ∏_{i=1}^n E[f_i(X_i)]."}
{"Major": "Statistics", "Term": "statistical inference", "Explanation": "Statistical inference is the process of drawing conclusions about an unknown population quantity θ ∈ Θ from data X1,...,Xn generated by a model M = {f(x|θ)}. It comprises estimation, hypothesis testing, and uncertainty quantification. In the frequentist framework, θ is fixed; inference relies on the sampling distribution of estimators and test statistics derived from the likelihood. An estimator θ̂(X) aims to estimate θ; desirable properties include unbiasedness (Eθ[θ̂]=θ), consistency (θ̂ → θ in probability), and efficiency (attaining the Cramér–Rao bound). A confidence set Cn(X) satisfies Pθ(θ ∈ Cn(X)) ≥ 1−α for all θ ∈ Θ. Hypothesis testing concerns H0: θ ∈ Θ0 vs H1: θ ∉ Θ0, with a test rejecting H0 when a statistic exceeds a critical region at level α. In the Bayesian framework, θ is treated as a random variable with prior π(θ); the posterior π(θ|x) ∝ f(x|θ)π(θ) yields credible sets and decision rules via a loss function. Model adequacy, identifiability, and sufficiency influence inference; asymptotic theory characterizes behavior as n → ∞."}
{"Major": "Statistics", "Term": "joint distribution", "Explanation": "Definition. Let (X, Y) be a random vector on a probability space. The joint distribution of (X, Y) is the probability measure μ on the Borel σ-algebra of R^2 given by μ(B) = P[(X, Y) ∈ B] for B ⊂ R^2. Equivalently, the joint distribution can be described by:\n\n- Joint CDF: F_{X,Y}(x, y) = P(X ≤ x, Y ≤ y). F_{X,Y} is nondecreasing in each argument, right-continuous, with limits F_{X,Y}(-∞, y) = 0, F_{X,Y}(∞, ∞) = 1.\n\n- Discrete case: joint pmf p_{X,Y}(x, y) = P(X = x, Y = y), with ∑_{x,y} p_{X,Y}(x, y) = 1. Marginals: p_X(x) = ∑_y p_{X,Y}(x, y), p_Y(y) = ∑_x p_{X,Y}(x, y). Independence ⇔ p_{X,Y}(x, y) = p_X(x) p_Y(y).\n\n- Continuous case: joint pdf f_{X,Y}(x, y) such that P((X, Y) ∈ A) = ∫∫_A f_{X,Y}(x, y) dx dy for Borel A, with ∫∫ f_{X,Y}(x, y) dx dy = 1. Marginals: f_X(x) = ∫ f_{X,Y}(x, y) dy, f_Y(y) = ∫ f_{X,Y}(x, y) dx. Independence ⇔ f_{X,Y}(x, y) = f_X(x) f_Y(y) a.e.\n\n- General case: μ may lack a density; the CDF F_{X,Y} always exists."}
{"Major": "Statistics", "Term": "random variable", "Explanation": "Let (Ω, F, P) be a probability space. A real-valued random variable is a measurable function X: Ω → ℝ, i.e., for every Borel set B ⊆ ℝ, X⁻¹(B) ∈ F. The law (distribution) of X is the pushforward measure μ_X on (ℝ, B(ℝ)) defined by μ_X(B) = P(X ∈ B) = P(X⁻¹(B)). The cumulative distribution function is F_X(x) = P(X ≤ x) = μ_X((−∞, x]). If ∫Ω |X| dP < ∞, the expectation E[X] = ∫Ω X(ω) dP(ω) exists; Var(X) = E[(X − E[X])²] when finite. A vector-valued random variable X: Ω → ℝ^k is defined similarly, with measurability to the Borel σ-algebra on ℝ^k, and its components form jointly distributed random variables. The distribution of X induces probabilistic attributes such as moments, distribution functions, and densities (when μ_X is absolutely continuous)."}
{"Major": "Statistics", "Term": "confidence interval (CI)", "Explanation": "Definition. Let θ ∈ Θ be the (possibly scalar or vector) parameter and X be a random sample with distribution Pθ. A (1−α) confidence interval (CI) for θ is a measurable random set CI(X) ⊆ Θ, typically written as [L(X), U(X)] for scalar θ or as a confidence region for θ ∈ Θ, such that for all θ ∈ Θ,\nPθ( θ ∈ CI(X) ) ≥ 1−α.\nIf the inequality holds with equality for all θ, the CI is exact; otherwise, it is conservative. The interval is random because it depends on the observed data; θ is fixed but unknown. The frequentist interpretation asserts that the procedure yields correct coverage in repeated sampling: when X is drawn from Pθ, the long-run proportion of trials in which CI(X) contains θ equals 1−α.\n\nExample. If σ^2 is known and X̄ ∼ N(μ, σ^2/n), a 100(1−α)% CI for μ is X̄ ± z1−α/2 · σ/√n. If σ^2 is unknown, replace σ with s and use the t distribution: X̄ ± tn−1,1−α/2 · s/√n.\n\nExtension. For θ ∈ R^d, CI becomes a random set with Pθ( θ ∈ CI(X) ) ≥ 1−α (e.g., ellipsoidal regions based on χ^2 or Wald-type constructions)."}
{"Major": "Statistics", "Term": "covariance", "Explanation": "Let X and Y be integrable random variables with finite means μ_X and μ_Y. The covariance is defined by Cov(X,Y) = E[(X−μ_X)(Y−μ_Y)]. Equivalently, Cov(X,Y) = E[XY] − μ_X μ_Y. For a k-dimensional random vector X with mean μ, Cov(X) = E[(X−μ)(X−μ)ᵀ], a symmetric positive semi-definite matrix.\n\nKey properties:\n- Bilinearity: Cov(aX+b, cY+d) = ac Cov(X,Y).\n- Symmetry: Cov(X,Y) = Cov(Y,X); Cov(X,X) = Var(X).\n- Independence implies zero covariance; conversely, zero covariance does not imply independence in general (except under joint normality).\n\nRelation to correlation:\n- ρ(X,Y) = Cov(X,Y) / (σ_X σ_Y), where σ_X = √Var(X), σ_Y = √Var(Y); |ρ| ≤ 1.\n\nEstimation:\n- Sample covariance s_{XY} = (1/(n−1)) ∑_{i=1}^n (X_i − X̄)(Y_i − Ȳ) estimates Cov(X,Y) under i.i.d. sampling.\n\nRemarks:\n- Covariance has units of (units of X)·(units of Y).\n- It measures linear association; it does not quantify nonlinear dependence."}
{"Major": "Statistics", "Term": "likelihood function", "Explanation": "In a statistical model with parameter θ ∈ Θ and observed data X = (X1, ..., Xn) drawn from a distribution with density fθ (for continuous data) or pmf pθ (for discrete data), the likelihood function is defined as L(θ; x) = ∏i fθ(xi) (or ∏i pθ(xi)) evaluated at the observed data x. More generally, L(θ; x) is the joint probability (or density) of the sample regarded as a function of θ, with x fixed; the function is not a probability distribution over θ. The domain is Θx = {θ ∈ Θ : fθ(xi) > 0 for all i} (or analogous condition for discrete cases).\n\nThe log-likelihood is ℓ(θ; x) = log L(θ; x) and is often employed due to numerical stability and additive aggregation. The maximum likelihood estimator θ̂ is any argument that maximizes L (equivalently ℓ) over Θx: θ̂ ∈ argmaxθ L(θ; x). The likelihood principle asserts that all inferential information about θ contained in the data is encapsulated by L(θ; x). Note that L is generally not normalized over θ. If a prior π(θ) is specified, the posterior is proportional to L(θ; x)π(θ)."}
{"Major": "Statistics", "Term": "probability measure", "Explanation": "Definition. Let Ω be a set, F ⊆ 2^Ω a σ-algebra. A probability measure is a function P: F → [0,1] such that\n- P(Ω) = 1, and\n- for any countable sequence {A_i} of pairwise disjoint sets in F, P(∪_{i=1}^∞ A_i) = ∑_{i=1}^∞ P(A_i) (countable additivity).\n\nConsequences: 0 ≤ P(A) ≤ 1 for all A ∈ F; P(∅) = 0; P(A^c) = 1 − P(A); and if A ⊆ B with A,B ∈ F, then P(A) ≤ P(B) (monotonicity). Continuity properties: if A_n ∈ F with A_n ↓ A then P(A_n) ↓ P(A); if A_n ↑ A then P(A_n) ↑ P(A).\n\nA probability space is the triple (Ω, F, P), consisting of the sample space Ω, a σ-algebra F of events, and a probability measure P on F."}
{"Major": "Statistics", "Term": "regression analysis", "Explanation": "Regression analysis denotes a family of statistical methods for estimating and interpreting the conditional distribution of a dependent variable Y given a set of regressors X. The principal object is the conditional mean μ(X) = E[Y|X]. In the canonical linear regression model, Y = Xβ + ε, with Y ∈ R^n, X ∈ R^{n×(p+1)} (including an intercept), β ∈ R^{p+1}, and ε satisfying E[ε|X] = 0 and Var(ε|X) = σ^2, together with no perfect multicollinearity. The ordinary least squares (OLS) estimator β̂ minimizes the sum of squared residuals ∑(Yi − Ŷi)^2, where Ŷi = Xi'β. Under the Gauss–Markov assumptions, β̂ is unbiased, consistent, and efficient among the class of linear unbiased estimators; with Gaussian errors, it is asymptotically normal, enabling t- and F-statistics for inference. Diagnostics involve residual analysis and tests for heteroskedasticity, autocorrelation, and multicollinearity. Extensions include nonlinear regression, generalized linear models, and nonlinear least squares; for binary outcomes, logistic regression; and for high-dimensional data, regularization (Ridge, Lasso). Core goals are estimation, hypothesis testing about β, and prediction of Y conditional on X, subject to correct model specification and exogeneity of regressors."}
{"Major": "Statistics", "Term": "causal study", "Explanation": "- Definition: A causal study is a study design and corresponding analysis aimed at estimating the causal effect of a treatment or exposure T on an outcome Y, distinguishing causal influence from mere association.\n\n- Framework: Adopt the potential outcomes (Rubin) model. For unit i, Y_i(t) denotes the outcome if T_i = t (t ∈ {0,1}). The individual causal effect is τ_i = Y_i(1) − Y_i(0). The estimand is typically the average causal effect (ACE) τ = E[τ_i] = E[Y_i(1) − Y_i(0)] or related estimands such as the average treatment effect on the treated (ATT).\n\n- Identification and assumptions: In randomized designs, independence between treatment and potential outcomes (randomization) plus consistency and SUTVA (stable unit treatment value and no interference) identify causal effects from observed data. In observational designs, identification requires ignorability (Y_i(0), Y_i(1) ⫫ T_i | X_i) and overlap; causal effects are recovered via adjustment for covariates X, propensity scores, or instrumental variables when ignorability fails.\n\n- Methods and design types: Randomized controlled trials; quasi-experimental designs (regression discontinuity, difference-in-differences, interrupted time series); propensity score matching/weighting; instrumental variable analyses.\n\n- Distinctions: A causal study targets internal validity (credible inference about causality) and clearly specifies the estimand and the assumptions required for identification."}
