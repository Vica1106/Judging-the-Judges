{"Major": "Artificial Intelligence", "Term": "Selective Linear Definite Clause Resolution", "Explanation": "Selective Linear Definite Clause Resolution (SLD resolution) is the procedural way logic programs (like Prolog) derive answers from rules.\n\n- Definite clause: every rule has exactly one positive conclusion (the head) and any number of subgoals (the body). Facts are rules with an empty body.\n- Linear: each step rewrites the current goal by resolving one subgoal with one rule, producing a new (smaller) list of subgoals. You follow a single path, not a web of simultaneous resolutions.\n- Selective: at each step you choose which subgoal to resolve next (usually the first one), rather than resolving all at once.\n\nHow it works in plain terms:\n1) Start with a query (a goal you want to prove).\n2) Pick one subgoal and find a rule whose head can match it (unify them).\n3) Replace that subgoal by the body of the rule (the new subgoals you must satisfy).\n4) Repeat until all subgoals are solved or you backtrack to try alternatives.\n\nThis yields a proof if the query follows from the rules. It underpins logic programming and is implemented in Prolog; it’s sound and, with appropriate search, complete for definite programs. It can, however, diverge or loop if the search space is infinite."}
{"Major": "Artificial Intelligence", "Term": "Big O notation", "Explanation": "Big O notation is a way to describe how the amount of work an algorithm needs grows as the input size grows. It focuses on the worst case and on growth rate, not exact times, and it ignores tiny constants.\n\nCommon classes:\n- O(1): constant work\n- O(log n): grows slowly (e.g., binary search)\n- O(n): linear\n- O(n log n): a bit more than linear\n- O(n^2): quadratic\n- O(2^n): exponential\n\nKey idea: drop constants and lower-order terms. If you double the input size n, an O(n) algorithm roughly doubles its work, while O(n^2) work quadruples.\n\nExamples:\n- Binary search on a sorted list is O(log n)\n- Scanning every item once is O(n)\n- Simple sorting like bubble sort is O(n^2)\n- Efficient sorts like mergesort are O(n log n)\n\nWhy it matters: Big O helps you compare how well algorithms scale with large data and lets you choose the more efficient option for big inputs, even if early numbers look similar."}
{"Major": "Artificial Intelligence", "Term": "neural machine translation (NMT)", "Explanation": "Neural machine translation (NMT) is a way for computers to translate text from one language to another using neural networks. Instead of hand-written rules, NMT learns from large collections of bilingual text. The basic idea: the model reads a whole sentence in the source language and then generates a sentence in the target language. It uses an encoder that converts the source sentence into a mathematical representation, and a decoder that turns that representation into translated words. An attention mechanism helps the decoder pick the right source words when producing each target word. Because it learns from examples, NMT often sounds more fluent and natural than older phrase-based systems. It can handle longer sentences and keep the meaning better. However, it can still make mistakes with idioms, rare words, or texts very different from what it saw during training, and it can reflect biases in its data. Training requires a lot of text and computing power, usually on GPUs."}
{"Major": "Artificial Intelligence", "Term": "NP-hardness", "Explanation": "NP-hardness is a way to describe how tough a problem is, in theoretical computer science. Here’s the plain version:\n\n- NP is the class of problems where, if someone gives you a candidate solution, you can check whether it’s correct quickly.\n- A problem is NP-hard if every problem in NP can be transformed into it in a short amount of time. In other words, solving an NP-hard problem quickly would let you solve all NP problems quickly.\n\nKey implications:\n- NP-hard does not necessarily mean the problem itself is in NP (you might not be able to check a solution quickly).\n- If an NP-hard problem is also in NP, it’s called NP-complete.\n- Many AI tasks are NP-hard (e.g., certain scheduling, graph coloring, some optimization problems). That’s why we use heuristics, approximations, or problem-specific tricks instead of guaranteed fast exact solutions.\n\nIn short: NP-hardness signals “as hard as the hardest problems we can even try to verify quickly,” so exact fast solutions are unlikely in general."}
{"Major": "Artificial Intelligence", "Term": "true quantified Boolean formula", "Explanation": "True quantified Boolean formula (TQBF) is a decision problem in logic. It asks: given a Boolean formula with variables that are quantified, is it true?\n\nWhat that means in plain language:\n- A Boolean formula uses true/false values and logical operations (and, or, not).\n- A quantified formula adds quantifiers in front of variables: exists (∃) or for all (∀).\n- The part after the quantifiers, called the matrix, is a normal Boolean formula without quantifiers.\n- The statement is true if, considering the quantifiers, the formula evaluates to true when the variables range over true/false.\n\nExamples:\n- ∃x ∀y (x ∨ y) is true: pick x = true; then no matter what y is, true ∨ y is true.\n- ∀x ∃y (x ∧ y) is false: if x = false, no y can make false ∧ y true.\n\nSo, TQBF asks whether a given quantified formula is true under standard semantics. It generalizes SAT (which is just ∃-quantified) and is known to be PSPACE-complete, i.e., very computationally challenging."}
{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "Algorithmic probability (also called Solomonoff probability) is a theoretical way to measure how likely a piece of data is, if you imagine a universal computer that is fed a completely random program.\n\nIntuition: some outputs can be produced by many short programs, while others need long, clunkier ones. If you assume random bits make up the program, shorter programs are far more common than long ones. So outputs generated by short programs tend to be more probable.\n\nIn plain terms: the probability of seeing a particular string is the sum of the probabilities of all programs that produce that string. Each program’s probability is how likely its bit string is when chosen at random, which doubles with each shorter step (roughly 2 to the power of the negative length of the program). The upshot is that simpler, shorter descriptions have higher algorithmic probability.\n\nNotes: this concept is not computable in general—you can’t determine the exact probability for arbitrary data. It’s a foundational idea linking simplicity to likelihood, used in AI foundations and in approximations like MDL (Minimum Description Length) and certain Bayesian approaches."}
{"Major": "Artificial Intelligence", "Term": "behavior informatics (BI)", "Explanation": "Behavior informatics (BI) is an interdisciplinary field that treats human behavior as information to study and shape. It sits at the crossroads of information science, psychology, sociology, and AI. The core idea is to collect actions, choices, and interactions—what people click, say, buy, or do—and turn them into data that can be analyzed to reveal patterns, motivations, and outcomes.\n\nBI uses AI and machine learning to model behavior, predict likely future actions, and test how different designs or prompts influence decisions. It aims to design better digital systems—personalized recommendations, adaptive user interfaces, and digital nudges—that help people learn, stay healthy, be safe, or work more productively.\n\nTypical workflow: gather behavioral data from sensors, logs, and apps; preprocess and analyze it with models; validate insights; and deploy improvements or interventions. Important considerations include privacy, ethics, and transparency, since BI involves sensitive information about people’s actions and choices."}
{"Major": "Artificial Intelligence", "Term": "big data", "Explanation": "Big data means data sets that are so large and complex that ordinary software can’t handle them easily. It isn’t just “a lot of data”—it’s data that is big, fast, and varied.\n\nThree main ideas:\n- Volume: huge amounts of information from many sources.\n- Velocity: data arriving or being updated very quickly.\n- Variety: many different kinds of data (text, photos, videos, sensor readings, logs).\n\nSometimes people add two more ideas: veracity (data quality and reliability) and value (useful insights).\n\nExamples: social media posts, online shopping logs, GPS data from phones, and readings from sensors in devices or machines.\n\nWhy it matters: with the right tools, big data helps us spot trends, predict what’ll happen, improve products and services, personalize experiences, detect problems, and advance science.\n\nHow it’s handled: companies use powerful computing systems that work across many computers, plus software that cleans, stores, and analyzes data, often with AI and machine learning.\n\nChallenges: privacy and security, data quality, bias, and figuring out how to govern and use data responsibly."}
{"Major": "Artificial Intelligence", "Term": "convolutional neural network", "Explanation": "Convolutional neural network (CNN) is a type of artificial neural network designed to work with images and other grid-like data. Instead of connecting every input to every neuron, a CNN uses small filters that slide (convolve) over the input image. Each filter looks for a simple pattern, like an edge or a corner, in a tiny patch of pixels. The same filter is reused across the whole image, which keeps the number of parameters manageable. After the convolution, an activation function adds nonlinearity, and then a pooling step reduces the image size while keeping the most important information. Stacking many convolutional and pooling layers lets the network build from simple features to more complex ones (textures, shapes, objects). The final layers are typically fully connected and output class probabilities. CNNs learn all the filters and weights automatically by training on labeled examples using backpropagation. They’re especially good for vision tasks because they respect the spatial structure of images and can handle different image sizes with some adjustments."}
{"Major": "Artificial Intelligence", "Term": "Darkforest", "Explanation": "Dark Forest isn’t a single formal AI term, but two common ways people use it in AI-related discussions.\n\n1) The Dark Forest (cosmology/fiction influence): From Liu Cixin’s idea, the universe is like a dark forest where civilizations must stay hidden to avoid being attacked. If you reveal yourself, you might become a target. In AI talk, this metaphor is used to suggest that as AI systems get more powerful, actors may hide their true capabilities or intentions and act cautiously (or aggressively) to avoid being exposed or exploited. The key idea: you can’t safely rely on knowing others’ plans, so silence and stealth can seem safer than openness.\n\n2) A metaphor for opaque AI safety and competition: Some use Dark Forest to describe multi-agent environments where participants conceal information, bluff, or act strategically. Safe design then emphasizes robust alignment, verification, governance, and conservative signaling—preparing for others who might be hidden or deceptive.\n\nIf you meant a specific project, company, or usage, tell me and I’ll tailor the explanation."}
