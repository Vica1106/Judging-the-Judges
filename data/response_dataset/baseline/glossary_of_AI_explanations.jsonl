{"Major": "Artificial Intelligence", "Term": "Selective Linear Definite Clause Resolution", "Explanation": "Selective Linear Definite Clause Resolution (SLD Resolution) is a method used to derive answers from a knowledge base of definite clauses (facts and rules).\n\nIn plain terms:\n- Definite clauses are like rules with a single positive conclusion and a body of conditions. Facts are rules with an empty body.\n- A query asks whether something is true given those rules and facts.\n- “Selective” means the system picks one subgoal (one part of the query) to work on at a time.\n- “Linear” means it resolves that one subgoal in sequence, never resolving many at once.\n- The system uses unification to match variables in rules with the current subgoal.\n- If the chosen subgoal can be satisfied by a rule or fact, that subgoal is replaced by the body of the rule (more subgoals appear). If a path turns out to be impossible, the system backtracks and tries alternatives.\n\nThis is the core execution method behind Prolog: it searches for a proof of the query by repeatedly selecting a subgoal, applying a rule, and backtracking when needed. Example: proving ancestor(X,Y) from parent facts and recursive rules."}
{"Major": "Artificial Intelligence", "Term": "Big O notation", "Explanation": "Big O notation is a way to describe how the amount of work an algorithm does grows as the input gets bigger. It’s about growth rate, not exact time, and it usually refers to the worst case, ignoring tiny constants.\n\nCommon examples:\n- O(1) — constant time: the work doesn’t change with input size.\n- O(log n) — logarithmic: grows slowly (like binary search).\n- O(n) — linear: grows in direct proportion to n (scanning a list).\n- O(n^2) — quadratic: grows faster with nested loops.\n- O(2^n) — exponential: doubles with each extra item, very fast growth.\n\nKey idea: as n gets large, we focus on the dominant term and drop constants, so O(n+5) = O(n).\n\nUse it to compare algorithms or data structures. Example: searching an unsorted list is O(n); with a sorted list and binary search you get O(log n). For AI, Big O helps gauge how well an approach scales with bigger datasets or models."}
{"Major": "Artificial Intelligence", "Term": "neural machine translation (NMT)", "Explanation": "Neural machine translation (NMT) is a way for computers to translate text from one language to another using neural networks. Instead of breaking text into little pieces and matching them with dictionaries, NMT learns to translate by looking at many example translations and predicting the most likely target sentence.\n\nIt uses a model called an encoder-decoder: the encoder reads the source sentence and turns it into a mathematical representation; the decoder then writes the translated sentence word by word. Modern NMT often uses a Transformer, which can pay attention to different words in the sentence at once, capturing context and word order better.\n\nTo train, you feed the model many pairs of sentences in two languages. After training, you can input a sentence in one language and get a fluent, whole-sentence translation. NMT improves fluency and coherence over older phrase-based methods but needs lots of data and computing power, and may struggle with rare words or languages with little data. It’s used in apps, websites, and devices for real-time or batch translation."}
{"Major": "Artificial Intelligence", "Term": "NP-hardness", "Explanation": "NP-hardness is a way to describe how tough a problem is for computers.\n\n- NP problems: these are problems where, if someone gives you a candidate solution, you can check quickly (in polynomial time) whether it’s correct.\n- NP-hard problems: these are at least as hard as the hardest problems in NP. In fact, any NP problem can be turned into solving an NP-hard problem in a reasonable amount of time (polynomial time). So, if you could solve an NP-hard problem fast, you could solve all NP problems fast.\n\nImportant points:\n- NP-hard does not have to be in NP itself. It might not even have quickly verifiable solutions (some are optimization tasks).\n- If a fast algorithm existed for an NP-hard problem, it would imply P = NP (a famous unsolved question).\n\nExamples: the decision version of the traveling salesman problem is NP-complete (in NP and NP-hard). Many optimization versions (like finding the absolute shortest route) are NP-hard.\n\nIn AI, NP-hardness explains why we use heuristics, approximations, or limit problem size: exact, guaranteed-fast solutions are unlikely for these problems in general."}
{"Major": "Artificial Intelligence", "Term": "true quantified Boolean formula", "Explanation": "A true quantified Boolean formula is a boolean formula that includes quantifiers over variables and evaluates to true.\n\n- What it is: A quantified boolean formula (QBF) puts exists (∃) or for all (∀) in front of variables. The question is: given this order of quantifiers and the inside statement (the matrix), is the whole formula true?\n\n- Why “true”: If the quantified statement holds under the rules of logic, the formula is true. Since a fully quantified formula has no free variables, it’s either true or false.\n\n- Simple example: ∀x ∃y (x ∨ y)\n  - For x = false, pick y = true to make (x ∨ y) true.\n  - For x = true, (x ∨ y) is true no matter what y is.\n  - So the whole formula is true.\n\n- Another example (false): ∃x ∀y (x ∧ y)\n  - If x = 0, x ∧ y is false for all y.\n  - If x = 1, ∀y (1 ∧ y) would require y = 1 for all y, which isn’t possible.\n\nQBF truth testing is a hard problem (PSPACE-complete) and generalizes SAT, useful for modeling planning and games."}
{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "Algorithmic probability is a theoretical way to measure how likely a piece of data is based on how easy it is to generate with a computer program.\n\nIdea in plain terms:\n- Imagine choosing a random computer program by flipping random bits (short programs are more likely because there are many more long ones).\n- Run that program on a universal computer and see what it outputs (and that it halts).\n- The probability that the output is a given string is called its algorithmic probability.\n\nKey implications:\n- Short, simple programs have higher probability. So outputs with simple patterns (like 101010… or repeating phrases) tend to be considered more probable than random-looking data.\n- This connects to Kolmogorov complexity: simpler strings have shorter descriptions, and higher algorithmic probability.\n\nRelation to AI theory:\n- It provides a universal prior for inductive reasoning (Solomonoff induction): simpler explanations are a priori more plausible.\n- It’s powerful conceptually but uncomputable in general (we can’t determine exact probabilities because we can’t tell if programs halt). In practice, people use approximations and model-selection ideas (MDL, Bayesian methods with simple models)."}
{"Major": "Artificial Intelligence", "Term": "behavior informatics (BI)", "Explanation": "Behavior informatics (BI) is the study of behavior using data. It treats human actions and reactions as something that can be observed, measured, and analyzed, then used to make better technology and policies.\n\nWhat BI does:\n- Collects behavioral data from apps, devices, social media, and environments (things people do, when, and where).\n- Builds models to understand patterns and routines (habits, decisions, preferences).\n- Uses those models to predict future behavior and to design systems that respond appropriately.\n\nHow it’s used:\n- Personalization: apps that adapt to your goals and routines.\n- Health and education: tailored coaching and feedback.\n- Safety and compliance: nudges and reminders to reduce risky behavior.\n- Smart environments and public policy: designing spaces and rules that guide behavior in beneficial ways.\n\nRelation to AI:\n- BI uses AI and machine learning to find patterns, predict actions, and automate helpful responses.\n- It aims to improve user welfare and outcomes, while balancing privacy and ethics.\n\nBottom line: BI turns behavior into actionable insights to build smarter, more responsive technologies."}
{"Major": "Artificial Intelligence", "Term": "big data", "Explanation": "Big data refers to extremely large and diverse sets of information that are hard to manage with traditional tools. It isn’t just “a lot of data”—it's data that comes from many sources (phones, sensors, online transactions, videos), in many formats (text, images, numbers) and often in real time. The challenge is to collect, store, search, and analyze this data to find useful patterns, trends, or predictions.\n\nAI uses big data to learn. The more high-quality data available, the better AI models can recognize patterns and make accurate decisions or forecasts. For example, data from weather sensors, social media, and GPS can help predict flight delays, tailor marketing offers, or map disease outbreaks.\n\nBig data also requires powerful software and hardware, as well as careful handling of privacy and data quality—not all data is reliable or appropriate to use. People often describe big data with the “3 Vs”: Volume (how much data), Velocity (how fast it arrives), and Variety (the different kinds of data), with Veracity and Value added as common considerations. In short, big data is the massive, fast, varied data that fuels modern AI and data-driven decisions."}
{"Major": "Artificial Intelligence", "Term": "convolutional neural network", "Explanation": "Convolutional neural network (CNN) is a type of AI model designed for images and other grid-like data. It learns by looking at small parts of the input and building up features layer by layer. The core idea is using filters (or kernels) that slide over the image. Each slide computes a simple calculation (a dot product) to produce a feature map that shows where a certain pattern appears, like an edge or a corner. The same filter is used across the whole image, which means the model uses far fewer parameters than a fully connected network. After several convolutional layers, the network combines simple features into more complex ones (textures, shapes) and, later, objects. Pooling layers reduce the size of the data and keep the most important signals, helping with shift tolerance. In the end, a few fully connected layers decide the image’s category or perform other tasks like locating objects. CNNs are standard for image classification, detection, and segmentation."}
{"Major": "Artificial Intelligence", "Term": "Darkforest", "Explanation": "Dark Forest (concept): A metaphor from Liu Cixin’s sci‑fi work used in AI safety. In a dark forest, every civilization is hidden and cautious: revealing your presence or power could invite attack, because you can’t know others’ intentions. So the safest move is to stay quiet and minimize signals.\n\nAI context: As AI systems become more capable, sharing too much about their goals, internals, or even their existence could enable misuse, manipulation, or dangerous competition. The dark forest idea suggests being careful about signaling—restricting who can access powerful tools, limiting disclosure of sensitive internals, and designing systems to be safe even if encountered by unknown actors. It underpins ideas like containment, robust alignment, incremental testing, and guarded deployment rather than open, unchecked release.\n\nLimitations: It’s a provocative metaphor, not a literal rule. Critics argue it may overstate risk or discourage beneficial collaboration and safety research."}
