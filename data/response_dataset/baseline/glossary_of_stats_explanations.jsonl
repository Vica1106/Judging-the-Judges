{"Major": "Statistics", "Term": "mutual independence", "Explanation": "Mutual independence means the events don’t influence each other, no matter which combination you look at. If you have events A1, A2, ..., Ak, they are mutually independent when for every nonempty subset S of {1,...,k}, the probability of all events in S happening is the product of their individual probabilities:\nP(∩_{i∈S} Ai) = ∏_{i∈S} P(Ai).\n\nIn words: knowing that some of them occurred gives you no information about any others, and this holds for every possible group.\n\nThis is stronger than pairwise independence, which only requires P(Ai ∩ Aj) = P(Ai)P(Aj) for every pair (i, j).\n\nExample: Toss three fair coins. Let A = “first is heads,” B = “second is heads,” C = “third is heads.” Each has probability 1/2, and any combination has probability equal to the product of the involved probabilities, so A, B, C are mutually independent.\n\nNote: You can have pairwise independence without mutual independence. For some sets of events, every pair looks independent, but all together they are not."}
{"Major": "Statistics", "Term": "statistical inference", "Explanation": "Statistical inference is using data from a sample to guess something about a larger group. Since you usually can’t measure everyone, you draw conclusions and say how confident you are in them. The main ideas are:\n\n- Point estimates: a single best guess (e.g., the sample average estimates the population average).\n- Confidence intervals: a range that’s likely to contain the true value (e.g., we’re 95% confident).\n- Hypothesis testing: deciding whether an observed result is likely due to chance or reflects a real difference or effect.\n\nInferences are probabilistic and depend on the data quality and the methods used. Example: surveying 1,000 people to estimate what share will vote for a candidate, with a margin of error."}
{"Major": "Statistics", "Term": "joint distribution", "Explanation": "Joint distribution means: a rule that lists the probabilities for every possible combination of outcomes of two or more random variables. It shows how the variables behave together (whether they influence each other) and lets you derive their individual behavior.\n\nDiscrete case:\n- If X and Y take values in finite sets, the joint PMF is p(x,y) = P(X = x, Y = y). All pairs have nonnegative probabilities and sum to 1.\n- Marginals: p_X(x) = ∑_y p(x,y); p_Y(y) = ∑_x p(x,y).\n- Independence: X and Y are independent if p(x,y) = p_X(x) p_Y(y) for all x,y.\n- Conditional: P(X = x | Y = y) = p(x,y) / p_Y(y) (when p_Y(y) > 0).\n\nContinuous case:\n- The joint PDF is f_{X,Y}(x,y) with probabilities by area: P(a ≤ X ≤ b, c ≤ Y ≤ d) = ∬ region f_{X,Y}(x,y) dx dy; total integral is 1.\n- Marginals: f_X(x) = ∫ f_{X,Y}(x,y) dy; f_Y(y) = ∫ f_{X,Y}(x,y) dx.\n- Independence: f_{X,Y}(x,y) = f_X(x) f_Y(y).\n- Conditional density: f_{X|Y}(x|y) = f_{X,Y}(x,y) / f_Y(y).\n\nExample: two fair dice have joint PMF 1/36 for each pair; they’re independent."}
{"Major": "Statistics", "Term": "random variable", "Explanation": "A random variable is a way to turn the randomness of a situation into a number you can work with. It’s a rule that assigns a numeric value to each possible outcome of a random process.\n\n- Examples:\n  - Roll a six-sided die. Let X be the random variable that equals the number shown (1 through 6). Each value has probability 1/6.\n  - Flip a coin and let Y be 1 if heads and 0 if tails.\n  - Measure rainfall. The random variable could be the amount in inches, which can be any nonnegative number (continuous).\n\n- Discrete vs. continuous:\n  - Discrete: takes finite or countable values (0, 1, 2, …).\n  - Continuous: can take any value in an interval (like any number of inches).\n\n- What you need from it: the distribution, which tells you how likely each value is. From it you can compute the average outcome (expected value) and how spread out the values are (variance/standard deviation).\n\nIn short, a random variable is a numerical summary of the result of a random process."}
{"Major": "Statistics", "Term": "confidence interval (CI)", "Explanation": "A confidence interval (CI) is a range made from your data that’s used to guess a population value (like an average) with a stated level of trust.\n\nHow it works: You collect data and compute an estimate (for example, the average). Because the data are just a sample, there’s uncertainty. The CI adds and subtracts a margin of error around that estimate to form a range.\n\nCommonly you’ll see a 95% CI. This means: if we repeated the study many times and made a new CI each time, about 95% of those intervals would contain the true population value. It doesn’t mean there’s a 95% chance the true value is in this one interval.\n\nWhat affects the width: more variability in the data, smaller samples, or a higher confidence level all make the interval wider; larger samples or less variability make it narrower.\n\nExample: If the sample average height is 170 cm with a margin of error of 4 cm, the 95% CI might be 166 to 174 cm. This suggests the true average height is likely between those numbers, with that level of confidence."}
{"Major": "Statistics", "Term": "covariance", "Explanation": "Covariance is a measure of how two variables change together.\n\n- If they tend to both be high or both be low at the same time, covariance is positive.\n- If one tends to be high when the other is low, covariance is negative.\n- If there’s no consistent pattern, covariance is around zero.\n\nImportant notes:\n- The size of covariance depends on the units you use for each variable, so its raw value isn’t easy to interpret by itself.\n- It only captures linear relationships (how they move together in a straight-line way). Two things can have strong a nonlinear relationship and still have a small or zero covariance.\n\nRelation to correlation: correlation is covariance divided by the product of the variables’ standard deviations. That standardizes the measure to a -1 to 1 scale, making it easier to compare.\n\nFormulas (brief):\n- Population: Cov(X,Y) = E[(X − μx)(Y − μy)]\n- Sample: Cov(X,Y) = [Σ (xi − x̄)(yi − ȳ)] / (n−1)\n\nExample: height and weight tend to rise together, so their covariance is positive."}
{"Major": "Statistics", "Term": "likelihood function", "Explanation": "A plain-language explanation (200 words or less):\n\nLikelihood function tells you how plausible different values of a model parameter are, given your observed data. It’s a function of the parameter θ (not of the data) built from the data you actually saw.\n\n- For independent data, L(θ) = product of the model’s probabilities (or densities) for each observation: L(θ) = ∏ f(x_i | θ).\n- It’s not the probability that θ is true. It’s a measure of how likely the observed data would be if θ were that value.\n- The common goal is to find the maximum likelihood estimate (MLE): the θ that makes the observed data most likely. We often maximize the log-likelihood because sums are easier to work with than products.\n- Example: you toss a coin 10 times and see 7 heads. If p is the probability of heads, the likelihood is L(p) = p^7 (1−p)^3. The p that maximizes L(p) is 0.7.\n- In Bayesian analysis, the likelihood is combined with a prior to form the posterior distribution. It also underpins model comparison through likelihood ratios, AIC, and BIC.\n\nIn short: the likelihood function connects observed data to possible parameter values, guiding estimation and inference."}
{"Major": "Statistics", "Term": "probability measure", "Explanation": "A probability measure is a rule that assigns to every possible event a number between 0 and 1 that says how likely that event is. An event is any set of outcomes you could observe in an experiment.\n\nKey ideas:\n- It’s defined on a collection of events that’s closed under taking combinations (a sigma-algebra).\n- The probability of the sure event (the whole sample space) is 1.\n- If several events can’t happen at the same time (mutually exclusive), the probability that one or the other happens is the sum of their probabilities. This additivity extends to any countable collection of disjoint events.\n\nIn simple terms, probabilities add up consistently and everything possible adds up to 1.\n\nExamples:\n- Fair coin: P({heads}) = 0.5.\n- Fair die: P({3}) = 1/6, and P({1,2,3,4,5,6}) = 1.\n- Continuous outcomes (like height): instead of listing outcomes, we use a probability density that integrates to 1.\n\nA probability measure is the mathematical foundation that lets us talk about how likely different events are and to compute probabilities for complex scenarios."}
{"Major": "Statistics", "Term": "regression analysis", "Explanation": "Regression analysis is a statistical method used to understand how one thing changes when another thing changes. It looks for a relationship between a dependent variable (what you want to predict or explain) and one or more independent variables (factors you think influence it). The result is a model you can use to predict the dependent variable from the inputs.\n\nIn simple linear regression, you fit a straight line: predicted_y = intercept + slope × x. The slope shows how much y tends to change when x changes. The model’s coefficients are estimated from data, usually by minimizing the sum of squared differences between observed and predicted values (least squares). You also get a sense of how strong the relationship is (R-squared) and how reliable the estimates are (standard errors, p-values).\n\nCommon uses: predicting house prices from size and location; forecasting sales from advertising spend. Important caveat: regression shows associations, not proven cause. Assumptions include linearity, consistent variance of errors, and independent observations."}
{"Major": "Statistics", "Term": "causal study", "Explanation": "A causal study tries to answer: does one thing cause another thing to change? It’s about cause-and-effect, not just whether two things happen together.\n\nHow it works:\n- In the strongest studies, researchers deliberately change something (an intervention) for some people and not for others, using random assignment. This is a randomized controlled trial.\n- If randomization isn’t possible, scientists use alternative designs or methods to imitate random assignment and control for other factors that could influence the outcome (confounders).\n\nKey idea: imagine what would have happened to the same people if they hadn’t received the intervention (a counterfactual). The difference in outcomes between groups is the causal effect, often summarized as an effect size (e.g., average treatment effect).\n\nImportant note: correlation is not causation. Two things can move together without one causing the other, especially if a third factor explains both.\n\nCommon goals: determine if a treatment, policy, or action actually produces a real change, and estimate how big that change is."}
