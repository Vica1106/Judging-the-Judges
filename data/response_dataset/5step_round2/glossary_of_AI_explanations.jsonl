{"Major": "Artificial Intelligence", "Term": "Selective Linear Definite Clause Resolution", "Explanation": "1) One-sentence definition:\nSelective Linear Definite Clause Resolution is a goal-directed method for deriving answers from Horn clauses by picking one subgoal at a time and resolving it with a rule that has a single positive head, producing a shorter subgoal list.\n\n2) Simple intuition with everyday example:\nThink of following a recipe or to-do list: pick the next task, consult a rule that explains how to do it, and replace that task with its smaller substeps. If a step works, you continue; if not, you back up.\n\n3) Key components and related concepts:\n- Facts and rules (definite/Horn clauses)\n- Goals and subgoals (current tasks to prove)\n- Unification/substitution (fit variables to match rules)\n- Selection rule (choose which subgoal to expand)\n- Derivation tree/backtracking (step-by-step plan with retries)\n\n4) Clear real-world analogy with mapping:\nAnalogy: cooking with a recipe book.\n- Goal → dish you want to cook\n- Definite clause head → the dish produced by a recipe line\n- Clause body → the substeps/ingredients needed\n- Selected literal → the next substep you pick\n- Unification → checking ingredients match the recipe\n- Substitution → filling in actual amounts\n- Backtracking → trying a different recipe if a step fails\n\n5) Common misconceptions and clarifications:\nMisconception: it’s just forward-chaining or greedy. Reality: it’s a goal-directed, backtracking search that resolves one subgoal at a time using unification; termination isn’t guaranteed and success gives substitutions rather than a simple true/false."}
{"Major": "Artificial Intelligence", "Term": "Big O notation", "Explanation": "1) One-sentence definition: Big O notation describes how an algorithm's runtime or memory grows as the input size grows.\n\n2) Simple intuition with everyday example: Imagine searching for a card in a pile of n cards by checking one by one; you’ll do about n/2 checks on average, and doubling the deck roughly doubles the work.\n\n3) Key components and related concepts:\n- Input size n; T(n) is the work (time/space) as a function of n\n- Big O = upper bound on growth\n- Common classes: O(1), O(log n), O(n), O(n log n), O(n^2), O(2^n)\n- Related ideas: Big Omega, Big Theta, average vs worst case, space complexity\n\n4) Real-world analogy with mapping: Library lookup\n- n = number of recipes/books\n- O(1) = instant exact lookup via a perfect index\n- O(log n) = use an organized index to halve the search space (binary-like)\n- O(n) = flip through items one by one\n- O(n log n) = sort first (n log n) then search efficiently\n- Mapping: growth class mirrors how many items you touch as n grows\n\n5) Common misconceptions and clarifications:\n- Not exact runtime; it’s an upper bound and asymptotic\n- Constants/hardware affect actual time but not the growth class\n- O(n^2) vs O(n log n): n^2 dominates for large n, but small-n behavior may differ\n- Big O can describe time or space, not both by default"}
{"Major": "Artificial Intelligence", "Term": "neural machine translation (NMT)", "Explanation": "1. One-sentence definition: NMT is a system that uses neural networks to translate text from one language to another.\n\n2. Simple intuition with everyday example: Think of a bilingual student who studies thousands of paired sentences and learns to express the same idea in a different language.\n\n3. Key components and related concepts:\n- Encoder: reads the source sentence and captures its meaning\n- Decoder: writes the translated sentence in the target language\n- Attention: helps focus on the right words while translating\n- Training data: large collections of sentence pairs\n- Evaluation and model types: quality metrics and options like multilingual vs. single-language models\n\n4. Clear real-world analogy with mapping:\nAnalogy: a translator using a recipe book.\n- Source sentence = original recipe\n- Encoder = reader who captures the flavors (meaning)\n- Attention = spotlight on key ingredients\n- Decoder = writer who drafts the recipe in the new language\n- Training data = many paired recipes\n- Output = translated recipe\n\n5. Common misconceptions and clarifications: A frequent misunderstanding is that NMT translates word-for-word. That’s wrong: it aims to convey meaning and natural phrasing, which can require reordering or choosing different expressions. The correct view: NMT learns patterns from data and may still need human review for critical accuracy."}
{"Major": "Artificial Intelligence", "Term": "NP-hardness", "Explanation": "1) One-sentence definition\nNP-hard means every problem whose solution can be quickly checked can be translated into this problem in polynomial time, so a fast solver for the NP-hard problem would fast-solve all those problems.\n\n2) Simple intuition with everyday example\nThink of one “ultimate” puzzle that can simulate any other difficult puzzle. If you could crack this universal puzzle quickly, you could solve any puzzle in that class quickly. You can check a proposed answer fast, but finding the answer from scratch is the hard part.\n\n3) Key components and related concepts\n- Reduction: transform any NP problem to the NP-hard problem in polynomial time.\n- Implication: a fast algorithm for the NP-hard problem would give fast algorithms for all NP problems.\n- NP vs NP-hard: NP-hard may not itself be verifiable quickly; some NP-hard problems aren’t in NP.\n- Related: NP-complete = NP-hard and in NP.\n\n4) Clear real-world analogy with mapping\nAnalogy: a universal puzzle box.\n- NP problems = many different puzzles you might be given.\n- NP-hard problem = the boss puzzle inside the box.\n- Reduction = translating any other puzzle into a version you feed to the boss puzzle.\n- Fast boss-solver = an algorithm that solves the boss puzzle quickly, thus solves all translated puzzles quickly.\n- Quick check = quickly verifying a candidate solution to any puzzle.\n\n5) Common misconceptions and clarifications\n- Misconception: NP-hard means unsolvable quickly. Wrong: some instances are easy; NP-hard refers to the worst-case difficulty.\n- Misconception: all NP-hard problems are in NP. Not necessarily.\n- Correct view: reductions show relative hardness; P=NP would make many of these easy."}
{"Major": "Artificial Intelligence", "Term": "true quantified Boolean formula", "Explanation": "1) One-sentence definition:\nA true quantified Boolean formula is a fully quantified statement of the form Q1 x1 Q2 x2 ... Qn xn : φ(x1,...,xn) where each Qi is ∃ or ∀, and the sentence is true under standard logic.\n\n2) Simple intuition with everyday example:\nThink of a two-player game: you choose some variables (exists), the opponent chooses others (forall), and you win if the final condition φ is satisfied no matter the opponent’s moves.\n\n3) Key components and related concepts:\n- Variables x1…xn\n- Quantifiers Q1…Qn in order (∃ or ∀)\n- Matrix φ, a Boolean formula over the x’s\n- Truth under adversarial evaluation (game-like)\n- Related ideas: SAT (one-shot) and QBF; TQBF is PSPACE-complete\n\n4) Clear real-world analogy with mapping:\nAnalogy: an escape-room puzzle. You set some levers (∃), the room master sets others (∀), and a final check φ decides if you escape.\nMapping: levers = variables x; your choices = ∃; master’s choices = ∀; final puzzle check = φ; escaping = formula true.\n\n5) Common misconceptions and clarifications:\n- Misconception: any ∀ makes it impossible. Correction: with a good strategy, some true alternating-quantifier formulas exist.\n- Misconception: it’s just SAT. Correction: QBF extends SAT with quantifiers; solving is PSPACE-complete, not equivalent to plain SAT."}
{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "1) One-sentence definition\nAlgorithmic probability is the likelihood that a randomly chosen computer program outputs a given string when run on a universal computer.\n\n2) Simple intuition with everyday example\nShort, simple rules are easier to write. If you pick a rule at random, you’re more likely to land on a short rule that produces a familiar result (like a catchy slogan) than a long, awkward one.\n\n3) Key components and related concepts\n- Random program: a uniformly chosen binary string\n- Universal computer: a machine that can run any program\n- Output string: the produced text or data\n- Weight by length: shorter programs contribute more to the probability\n- Related ideas: Kolmogorov complexity (shortest description) and Solomonoff induction (formal prior over explanations)\n\n4) Real-world analogy with mapping\nAnalogy: a kitchen with a giant recipe book. You pick a recipe at random and cook it in a magical oven; the dish you get is the output.\n- Recipe = program\n- Dish = output string\n- Oven = universal computer\n- Probability of a dish = sum of weights of all recipes that produce it (short recipes weigh more, 2^(-length))\n\n5) Common misconceptions and clarifications\n- Not a statement about real-world frequencies; it’s a theoretical prior used in reasoning.\n- Short outputs aren’t guaranteed correct or true.\n- In practice it’s often uncomputable; it’s a guiding concept, not a recipe you can run."}
{"Major": "Artificial Intelligence", "Term": "behavior informatics (BI)", "Explanation": "1) One-sentence definition\nBehavior informatics (BI) is the study of collecting and analyzing data about actions and interactions to understand behavior and improve technology.\n\n2) Simple intuition with everyday example\nLike a fitness tracker that learns your routines to suggest better workouts, BI learns patterns in how people and systems act to make apps smarter and more useful.\n\n3) Key components and related concepts\n- Data about behavior (actions, choices, sequences)\n- Measurement and collection\n- Analysis and modeling (patterns, predictions)\n- Design changes, ethics, and privacy\n- Related ideas: data science, machine learning, human–computer interaction\n\n4) Clear real-world analogy with mapping\nAnalogy: a smart personal assistant that observes your daily habits to tailor reminders and suggestions.\n- Your actions/times = BI data collection\n- The learning engine = BI analytics/models\n- Personalized suggestions = BI outputs\n- Your responses = feedback/learning loop\n- Updated features or routines = design decisions guided by BI\n\n5) Common misconceptions and clarifications\nMisconception: BI is just AI/prediction.\nWhy wrong: BI also focuses on understanding behavior to inform design, decisions, and ethics/privacy.\nCorrect view: BI combines data, analysis, and design to improve how tools respond to people."}
{"Major": "Artificial Intelligence", "Term": "big data", "Explanation": "1) One-sentence definition: Big data are extremely large and complex data sets that require special tools to store, process, and analyze.\n\n2) Simple intuition with everyday example: Imagine trying to learn what people want by looking at every post, click, and photo from billions of users—too much for a simple spreadsheet, but doable with distributed systems.\n\n3) Key components and related concepts:\n- Volume: huge data sizes\n- Velocity: fast data flow\n- Variety: many data types\n- Veracity: data quality/trust\n- Value: useful insights\n- Related ideas: distributed computing (e.g., Hadoop/Spark) and data science\n\n4) Clear real-world analogy with mapping:\nAnalogy: a city-wide library of digital actions.\n- Volume → shelves of books (how much data)\n- Velocity → new items arriving all day (data speed)\n- Variety → different formats (texts, videos, logs)\n- Veracity → accuracy and reliability of items\n- Value → useful conclusions you can draw\n\n5) Common misconceptions and clarifications:\nMisconception: More data automatically means better results. Why it’s wrong: without clean data and smart methods, extra data can clutter analysis. Correct view: data quality and good analytics matter; big data helps reveal patterns when used properly."}
{"Major": "Artificial Intelligence", "Term": "convolutional neural network", "Explanation": "1) One-sentence definition:\nA convolutional neural network is an artificial neural network designed to recognize patterns in images by sliding small filters across the image to detect features and combine them to identify objects.\n\n2) Simple intuition with everyday example:\nThink of using a small stamp to scan a picture. You slide it across to reveal repeating patterns; after several stamps (layers) you understand the whole scene.\n\n3) Key components and related concepts:\n- Convolutional layers (filters)\n- Activation (e.g., ReLU)\n- Pooling (downsampling)\n- Depth (many layers)\n- Weight sharing (same filter across the image)\n- Fully connected output layer\n- Training with labeled data and backpropagation\n- Input channels (RGB) and padding/stride\n\n4) Clear real-world analogy with mapping:\nAnalogy: photo inspection in a factory.\n- Image = the photo you’re classifying\n- Filters/stamps = convolutional kernels\n- Convolution step = stamping the photo across all positions\n- Feature maps = stamped pattern clues\n- Pooling = summarizing nearby clues\n- Deep stack of layers = multiple rounds of stamping and summarizing\n- Fully connected classifier = final inspector assigns the label\n\n5) Common misconceptions and clarifications:\n- Misconception: filters are hand-designed. Correct: they are learned from data during training.\n- Misconception: more layers always mean better performance. Correct: depends on data, regularization, and training quality."}
{"Major": "Artificial Intelligence", "Term": "Darkforest", "Explanation": "1) One-sentence definition: Dark Forest is the idea that AI actors in a competitive, uncertain landscape may conceal their true capabilities and intentions to avoid being attacked or copied, creating a tense, low-trust environment.\n\n2) Simple intuition with everyday example: It’s like players in a high-stakes poker game who stay quiet about their strength—revealing a big hand could invite others to bluff, copy, or strike first, so everyone acts cautiously.\n\n3) Key components and related concepts:\n- Multiple, competing AI actors\n- Incomplete information about others’ power and goals\n- Incentives to conceal capabilities and plans\n- Risk of miscoordination and arms-race dynamics\n- Signals, safety policies, and governance as counterbalances\n- Related ideas: game theory, signaling, adversarial AI, cooperation vs. competition\n\n4) Clear real-world analogy with mapping:\nAnalogy: a large, silent forest where many intelligent actors operate.\n- Forest = AI ecosystem with many organizations\n- Darkness = incomplete observability of others’ power and intent\n- Trees/creatures = individual AI teams or systems\n- Hunters/footsteps = competing agents and signals of capability\n- Breakthroughs = powerful new abilities\n- Silence = strategic non-disclosure\nMapping to technical: hidden capabilities, uncertain incentives, and disclosure decisions shape safety, collaboration, and governance in real-world AI development.\n\n5) Common misconceptions and clarifications:\n- Misconception: “Dark Forest means inevitable conflict.” Why wrong: it’s a model of incentives, not destiny; outcomes depend on governance and cooperation. Correct perspective: risk rises with opacity; better coordination can reduce harm.\n- Misconception: “It’s only about military use.” Why wrong: affects all competitive AI settings (economics, safety, standards). Correct perspective: disclosure and safeguards matter across domains.\n- Misconception: “Transparency fixes everything.” Why wrong: transparency has tradeoffs; must balance disclosure with security and care. Correct perspective: thoughtful signaling and governance are key."}
