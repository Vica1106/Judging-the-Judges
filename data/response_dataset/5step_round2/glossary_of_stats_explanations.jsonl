{"Major": "Statistics", "Term": "mutual independence", "Explanation": "1. One-sentence definition:\nMutual independence means the outcome of each variable provides no information about any other, so the joint probability equals the product of the individual probabilities.\n\n2. Simple intuition with everyday example:\nThink of three fair coins flipped at once; knowing the result of one flip doesn’t change the chances for the other flips.\n\n3. Key components and related concepts:\n- For every possible values x1,…,xn: P(X1=x1,…,Xn=xn) = P(X1=x1)…P(Xn=xn)\n- This must hold for all subsets of variables\n- Implies pairwise independence, but pairwise independence alone is not enough\n- Variables can have different distributions\n\n4. Clear real-world analogy with mapping:\nAnalogy: three separate dice or coins rolled together.\n- Each die/coin corresponds to a random variable X1, X2, X3.\n- The marginal probability is the chance of each face or outcome on a single die/coin.\n- Independence means P(triple) = P(X1=x1)·P(X2=x2)·P(X3=x3) for every triple.\n- Knowing the result of one die/coin doesn’t change the others’ chances.\n- If one die is biased, independence still uses its actual marginal probabilities.\n\n5. Common misconceptions and clarifications:\n- Misconception: pairwise independence implies mutual independence. Why wrong: three variables can be pairwise independent yet not mutually independent (example: X1, X2, X3 with X1⊕X2 = X3).\n- Misconception: independence requires identical distributions. Why wrong: they can have different distributions; independence is about factorization, not sameness."}
{"Major": "Statistics", "Term": "statistical inference", "Explanation": "1) One-sentence definition\nStatistical inference is the process of using data from a sample to draw conclusions about a larger population.\n\n2) Simple intuition with everyday example\nIntuition: taste a spoonful of soup to judge the whole pot’s saltiness. If you survey 100 students about study time, you infer the average for all students, with some uncertainty.\n\n3) Key components and related concepts\n- Population vs. sample\n- Parameter vs. statistic\n- Estimation and confidence intervals\n- Hypothesis testing and p-values\n- Model assumptions and uncertainty\n\n4) Clear real-world analogy with mapping\nAnalogy: tasting a spoonful of soup to judge the whole pot.\nMapping:\n- Pot = population\n- Spoonful = sample\n- Saltiness of pot = population parameter\n- Saltiness in spoonful = sample statistic\n- Inferring pot saltiness = statistical inference\n- Confidence interval = plausible range for pot saltiness\n- Hypothesis test (is it under-salted?) = decision about pot saltiness\n- Recipe/mixing assumptions = model assumptions\n\n5) Common misconceptions and clarifications\n- Misconception: inference gives exact truth. Clarification: it provides probabilistic conclusions with uncertainty.\n- Misconception: bigger sample always fixes it. Clarification: helps, but validity also depends on design and representativeness.\n- Misconception: p-values prove effects. Clarification: they indicate compatibility with a hypothesis, not proof."}
{"Major": "Statistics", "Term": "joint distribution", "Explanation": "1. One-sentence definition: The joint distribution of two random variables X and Y gives the probability of every pair of outcomes (x,y), and, in the continuous case, their density.\n\n2. Simple intuition with everyday example: It’s like tracking how two things co-vary; roll two dice and note every pair (5,2), (3,6), etc.\n\n3. Key components and related concepts:\n- Variables X, Y\n- Joint pmf p(x,y) for discrete, or joint pdf f(x,y) for continuous\n- Normalization: sum or integral over all x,y equals 1\n- Marginals pX(x), pY(y)\n- Conditional distributions p(Y|X) or p(X|Y)\n- Independence: p(x,y) = pX(x) pY(y) for all x,y\n\n4. Clear real-world analogy with mapping:\nAnalogy: two dice on a grid. Each cell is a pair (x,y); the cell value is p(x,y). Row sums give pX, column sums give pY. Y given X is the row-normalized cell. If the grid equals the outer product of its marginals, X and Y are independent.\n\n5. Common misconceptions and clarifications:\n- A joint distribution is a single number. It’s a function of pairs that assigns probabilities.\n- Independence doesn’t mean no link in real life; it means p(x,y)=pX(x)pY(y) for all x,y.\n- It covers both discrete (pmf) and continuous (density); the continuous case uses integration, not summation."}
{"Major": "Statistics", "Term": "random variable", "Explanation": "1) One-sentence definition:\nA random variable is a function that assigns a numeric value to each possible outcome of a random process.\n\n2) Simple intuition with everyday example:\nThink of rolling a six‑sided die: the process is the roll, and the random variable X is the number that lands face up—a single numeric result.\n\n3) Key components and related concepts:\nSample space (all outcomes); the rule X that maps outcomes to numbers; the range of X (its possible values); the distribution P(X=value) over those values; and summaries like the expectation (mean) and variance.\n\n4) Clear real-world analogy with mapping:\nAnalogy: rolling a die.\n- Process: rolling the die.\n- Outcome ω: the face that lands up (1–6).\n- X(ω): the numeric value shown (the same 1–6 here).\n- Distribution: each value has probability 1/6.\n- Realized value: the number you observe on one roll.\n- Long-run: averaging X over many rolls gives the expected value.\n\n5) Common misconceptions and clarifications:\n- X is the probability: wrong. X is the mapping; probabilities live in its distribution.\n- X must be continuous: wrong. X can be discrete (like 1–6) or continuous (like height).\n- You only observe one value per trial: wrong. X can take many values; we summarize with its mean, variance, etc."}
{"Major": "Statistics", "Term": "confidence interval (CI)", "Explanation": "1) One-sentence definition: A confidence interval is a range of numbers calculated from sample data that is likely to contain the true value of a population parameter, given a stated confidence level (for example, 95%).\n\n2) Simple intuition with everyday example: It’s like a weather forecast window for a population value—if we repeated the study many times, about 95% of those windows would cover the true average.\n\n3) Key components and related concepts:\n- Sample statistic (e.g., mean or proportion) that starts the interval\n- Margin of error (how wide the window is)\n- Confidence level (e.g., 90%, 95%)\n- Lower and upper bounds (the ends of the interval)\n- Related ideas: sampling distribution, standard error; larger samples give narrower intervals\n\n4) Clear real-world analogy with mapping:\nAnalogy: a dartboard circle around your average throw.\n- True mean ↔ bullseye\n- Sample mean (center of your darts) ↔ center of the circle\n- Margin of error / radius ↔ circle radius\n- Confidence level (e.g., 95%) ↔ how often the circle would catch the bullseye in repeated plays\n- Computed interval ↔ the drawn circle for your data\n\n5) Common misconceptions and clarifications:\nMisconception: “There’s a 95% chance the true mean lies in this interval.”\nWhy wrong: the true mean is fixed; the interval either contains it or not. The 95% refers to the method's long-run success rate. Also: a wider interval is less precise, not more accurate, for the given data."}
{"Major": "Statistics", "Term": "covariance", "Explanation": "1. One-sentence definition\nCovariance is a measure of how two variables move together: positive means they tend to increase together; negative means one tends to decrease when the other increases.\n\n2. Simple intuition with everyday example\nIntuition: hours studied and exam score. More study usually links to higher scores, so covariance is positive; if scores don’t follow study, it’s near zero.\n\n3. Key components and related concepts\n- X and Y\n- deviations from their means\n- product of the deviations\n- average of those products\n- sign = direction; magnitude = strength (units matter)\n- related: correlation (standardized); independence (zero covariance ≠ independence)\n\n4. Clear real-world analogy with mapping\nAnalogy: two dancers. Each day note how far each strays from their spot. Covariance is the average of the product of those strays. Mapping: A=X; B=Y; stray=deviation; product=joint wandering; same-direction → positive; opposite → negative; random → near zero.\n\n5. Common misconceptions and clarifications\n- Zero covariance means no relationship: wrong; could be nonlinear.\n- Covariance vs correlation: covariance has units; correlation is unitless and ranges -1 to 1.\n- Covariance implies causation: wrong; shows association, not cause."}
{"Major": "Statistics", "Term": "likelihood function", "Explanation": "1) One-sentence definition: The likelihood function is the function that assigns to each possible parameter value the probability of observing the data you actually collected.\n\n2) Simple intuition with everyday example: If you flip a biased coin and observe 7 heads in 10 flips, the likelihood tells, for each bias p, how plausible that result would be.\n\n3) Key components and related concepts: - Parameter values; - Observed data; - Statistical model P(D|theta); - Likelihood L(theta) = P(D|theta); - Maximum Likelihood Estimator (theta_hat) = value that maximizes L; - Not P(theta) (not the probability of the parameter itself).\n\n4) Clear real-world analogy with mapping: Analogy: testing a coin bias by trying different p and seeing how likely the observed result is. Mapping: p = coin bias (parameter); D = observed sequence (e.g., 7 heads, 3 tails); L(p) = probability of observing D given p; p-hat = bias that makes D most likely; use L(p) to compare biases.\n\n5) Common misconceptions and clarifications: Misconception: likelihood equals the probability that the parameter is true. Wrong: likelihood is a function of the parameter for fixed data; it ranks how well each parameter explains the data. The correct view: higher likelihood means more support for that parameter value given the data."}
{"Major": "Statistics", "Term": "probability measure", "Explanation": "1) One-sentence definition:\nA probability measure is a rule that assigns to every event (a set of outcomes) a number between 0 and 1 that represents how likely the event is.\n\n2) Simple intuition with everyday example:\nIntuition: With a bag of 30 red, 20 blue, and 50 green balls, P(red)=0.30, P(blue)=0.20, and P(red or blue)=0.50; the numbers add up consistently.\n\n3) Key components and related concepts:\n- Sample space: all possible outcomes.\n- Events: subsets of outcomes (e.g., red balls).\n- Probabilities: numbers in [0,1], with P(∅)=0 and P(all outcomes)=1.\n- Additivity: for disjoint events, P(A∪B)=P(A)+P(B).\n- Related ideas: probability distribution, random variable, expectation.\n\n4) Clear real-world analogy with mapping:\nAnalogy: a bag of 100 balls (30 red, 20 blue, 50 green).\n- Outcomes = individual balls; Events = color groups (red, blue, green).\n- P(color) = count(color)/100.\n- Disjoint events: red and blue don’t overlap; P(red or blue)=P(red)+P(blue).\n- Whole space: red+blue+green covers all balls; P(all colors)=1.\n\n5) Common misconceptions and clarifications:\nMisconception: P(A) + P(B) = P(A∪B) for any A,B.\nCorrection: Only if A and B are disjoint; in general P(A∪B)=P(A)+P(B)−P(A∩B)."}
{"Major": "Statistics", "Term": "regression analysis", "Explanation": "1) One-sentence definition\nRegression analysis is a method for describing and predicting how a dependent variable changes when one or more independent variables change.\n\n2) Simple intuition with everyday example\nThink of predicting your exam score from hours studied: you look for a clear, best-fitting line that shows how scores tend to rise with more study, so you can estimate scores for new study plans.\n\n3) Key components and related concepts\n- Outcome (dependent) variable: what you measure (e.g., exam score)\n- Predictor(s) (independent variables): factors that explain changes (e.g., hours studied)\n- Relationship: the line or curve that summarizes the average change\n- Coefficients: slope (change per unit) and intercept (baseline)\n- Predictions and residuals: forecasted values and their errors\n- Fit and assumptions: how well the model explains data; linearity, consistent errors, etc.\n\n4) Real-world analogy with mapping\nAnalogy: predicting travel time from distance. \n- Distance (x) = predictor\n- Travel time (y) = outcome\n- Best-fit line = prediction rule\n- Intercept = fixed start time\n- Slope = extra time per mile\n- Data points = actual trips\n- Predictions = planned arrival times\n- Residuals = differences between planned and actual times\n- R-squared = how much distance explains travel time\n\n5) Common misconceptions and clarifications\n- Correlation ≠ causation: regression shows association, not proof of cause.\n- Higher R-squared ≠ truth: it’s a fit measure, not a final truth.\n- Outliers can distort results: check and address them.\n- More predictors ≠ better: risk of overfitting; aim for meaningful, parsimonious models."}
{"Major": "Statistics", "Term": "causal study", "Explanation": "1) One-sentence definition\nA causal study is research designed to show that a cause leads to an effect, not just that two things are related.\n\n2) Simple intuition with everyday example\nIntuition: if watering plants more leads to taller growth, you’re testing a cause, not merely a link; you try to keep sunlight and soil constant.\n\n3) Key components and related concepts\n- Treatment (the potential cause)\n- Outcome (the measured effect)\n- Control or comparison group\n- Random assignment or careful design\n- Temporal order (cause precedes effect)\n- Controlling confounders and bias\n\n4) Clear real-world analogy with mapping\nAnalogy: a plant growth test with fertilizer.\nMapping:\n- Fertilizer amount → treatment\n- Plant growth → outcome\n- Fertilizer vs none → treated vs control\n- Randomly assign fertilizer → randomization\n- Keep soil/light/water constant → confounders controlled\n- Apply before measuring growth → temporality\n- Consistent results → causal inference\n\n5) Common misconceptions and clarifications\n- Misconception: correlation equals causation. Why wrong: may be confounded or coincidental.\n- Clarification: with proper design/assumptions, causal studies estimate A’s effect; conclusions are probabilistic and require replication."}
