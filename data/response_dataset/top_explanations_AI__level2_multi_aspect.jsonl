{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "Algorithmic probability is a way to measure how likely a data pattern is to be produced by a computer program. Think of all possible small programs that could output the pattern. If a pattern can be generated by a short (simple) program, it gets a higher probability; longer, more complex programs contribute less. In short, simpler explanations are considered more probable.\n\nReal-world example: The string \"0101010101\" can be produced by a tiny program like “print 01 five times.” A truly random looking string, with no simple rule, needs a long program and thus has a much smaller probability. So, simple, regular patterns tend to have higher algorithmic probability than random noise.\n\nWhy it matters: It provides a theoretical way to think about prediction and learning—favoring simple, compressible explanations (Occam’s razor). It underpins universal induction and model selection in AI. Although exactly calculating it is impractical for long data, the idea guides how we judge explanations, build priors, and design algorithms that seek the simplest good-fit models."}
{"Major": "Artificial Intelligence", "Term": "attributional calculus", "Explanation": "Attributional calculus (in simple terms) is a lightweight way to explain a result by breaking it down into how much each input factor contributed. It uses straightforward, rule-based steps to assign shares of the outcome to different causes.\n\n1) Basic meaning\n- It answers: “What part of the result came from input A, input B, etc.?” often using additive rules so the contributions add up to the final outcome.\n\n2) Simple real-world example\n- If an AI helps decide loan approval using factors like credit score, income, and debt, attributional calculus would estimate how much each factor pushed the decision toward “approved” or “not approved.” For a test score example: score = base + 0.5*study + 0.3*sleep + 0.2*attendance, with each term showing its contribution.\n\n3) Why it is important\n- Improves transparency: users can see why a result happened.\n- Aids debugging and fairness: reveals which factors drive decisions and whether any bias exists.\n- Builds trust: explanations help people understand and accept AI outcomes."}
{"Major": "Artificial Intelligence", "Term": "statistical relational learning (SRL)", "Explanation": "- Basic meaning: SRL combines statistics with relations. It models not only attributes of objects (like numbers or categories) but also how those objects are connected to each other, using probabilistic rules or graphs. It captures uncertainty and the structure of the world at once.\n\n- Simple real-world example: In a social network, SRL can predict which new friendships will form by looking at who is friends with whom, shared classes or interests, and past friendship patterns. It uses both people’s attributes and the network of relationships.\n\n- Why it’s important: Many real problems involve connected data and uncertainty. SRL lets us make better predictions in areas like social networks, biology (protein interactions), medicine, fraud detection, and language understanding by leveraging both relationships and variability, rather than treating each item independently."}
{"Major": "Artificial Intelligence", "Term": "metabolic network reconstruction and simulation", "Explanation": "Metabolic network reconstruction and simulation\n\n1) Basic meaning\nIt’s a way to turn a cell’s chemistry into a computable map. The map shows which chemical reactions happen and how molecules flow between them. Reconstruction uses data from the organism’s genes, experiments, and scientific papers, while simulation runs computer calculations to predict what happens under different conditions (which substances build up, which pathways are used, and how fast reactions go).\n\n2) Simple real-world example\nIn yeast used for brewing or biofuel, scientists build a metabolic map of how sugar becomes ethanol and other byproducts. They then simulate growth with different sugar amounts and oxygen levels. The model can suggest gene tweaks or nutrient changes to boost ethanol yield before doing lab tests.\n\n3) Why it is important\nIt helps design and optimize microbes and processes for medicine, fuels, and chemicals; enhances understanding of diseases by comparing normal and altered metabolism; and reduces costly, time-consuming experiments by guiding where to focus efforts. It also supports AI-driven decision making by integrating diverse data sources."}
{"Major": "Artificial Intelligence", "Term": "dynamic epistemic logic (DEL)", "Explanation": "Dynamic Epistemic Logic (DEL)\n\n- Basic meaning: DEL is a formal way to study how the knowledge and beliefs of intelligent agents change when events happen—especially information exchanges (announcements, messages) or observations. It combines “who knows what” with how updates occur.\n\n- Simple real-world example: Two travelers, Ana and Ben, must pick a restaurant from Italian, Chinese, or Mexican. Initially, Ana is unsure; Ben knows the list. A public announcement from a host—“The Italian option is still available”—updates both users’ knowledge, narrowing possibilities. If the host’s message is private to Ana, only Ana updates; Ben stays as he was. DEL can model these public versus private updates precisely.\n\n- Why it’s important: In AI, DEL helps agents reason about others’ knowledge, anticipate actions, and plan dialogues or strategies in multi-agent settings. It supports designing secure communication, coordinating teams, and planning under uncertainty where what others know matters."}
{"Major": "Artificial Intelligence", "Term": "neural Turing machine (NTM)", "Explanation": "Neural Turing Machine (NTM)\n\n- What it is: A neural network plus a differentiable external memory. The network processes input and also reads from and writes to memory like a tiny computer RAM, using soft attention so everything can be trained with gradient descent.\n\n- Simple real-world example: A voice assistant that must remember a spoken shopping list. The NTM writes each item into memory and later reads them back in the right order. It’s also used to learn simple algorithms, like sorting a short list by storing numbers in memory and then retrieving them in sorted order.\n\n- Why it matters: Standard neural nets are strong at pattern recognition but struggle with tasks that require keeping track of data over time or following steps like an algorithm. NTMs give AI a way to learn memory-augmented, program-like behavior, enabling tasks such as copying, sorting, or planning—bridging perception and structured reasoning and inspiring further memory-based models."}
{"Major": "Artificial Intelligence", "Term": "answer set programming (ASP)", "Explanation": "Answer Set Programming (ASP) for a non-expert\n\n- What it is: ASP is a declarative way to solve hard problems. You describe the problem with simple rules, facts, and constraints, and a computer solver finds complete solutions called “answer sets” that satisfy everything.\n\n- Simple real-world example: School timetable. Facts: which teachers exist, which classes must be taught, and when. Rules: each class must be taught by exactly one available teacher; a teacher can’t teach two classes at the same time; some teachers can’t teach certain subjects; optionally prefer certain time slots. The solver outputs valid timetables that meet all constraints (and can even show alternative valid options).\n\n- Why it’s important: Real problems often involve many rules and trade-offs (schedules, planning, resource allocation). ASP lets you encode these constraints clearly without writing complex search code, and it can efficiently find solutions or all possible solutions, helping with decision-making, automation, and optimization."}
{"Major": "Artificial Intelligence", "Term": "NP-completeness", "Explanation": "NP-completeness (for non-experts)\n\n- Basic meaning: NP problems are those where a proposed solution can be checked quickly. NP-complete problems are the hardest in NP: every NP problem can be transformed into an NP-complete one. If you could solve any NP-complete problem quickly (in polynomial time), you could solve all NP problems quickly. Whether that’s actually possible is the famous P vs NP question.\n\n- Simple real-world example: Subset sum. Given numbers like 3, 7, 9, 12 and a target 15, is there a subset that adds up to 15? Yes (3 + 12). Verifying a given subset works is quick, but finding the right subset among many possibilities becomes hard as the list grows.\n\n- Why it’s important: It explains why many practical problems (scheduling, packing, route planning) are easy to describe but hard to solve exactly for large sizes. It motivates using approximate methods, heuristics, or special cases, and helps researchers understand what kinds of improvements are possible."}
{"Major": "Artificial Intelligence", "Term": "partially observable Markov decision process (POMDP)", "Explanation": "- Basic meaning: A POMDP is a framework for deciding what to do when you can’t see the world perfectly. It extends simple decision models by allowing observations that only partly reveal the true state. You have states, actions, transitions, rewards, and noisy observations. Since you don’t know the exact state, you keep a belief—a probability distribution over states—and choose actions based on that belief.\n\n- Simple real-world example: A robot vacuum cleaner. The true state is which rooms are dirty and where obstacles are. Its sensors give imperfect readings, so it never knows the exact state. It updates its belief about dirt in each room and picks where to clean next to maximize cleanliness while saving battery.\n\n- Why it’s important: Many real decisions are made with incomplete information. POMDPs provide a principled way to plan under uncertainty, balancing exploring what you don’t know with using what you think you know. They’re used in robotics, autonomous systems, medicine, and finance."}
{"Major": "Artificial Intelligence", "Term": "quantum computing", "Explanation": "- Basic meaning: Quantum computing uses quantum bits (qubits) that can be 0, 1, or both at once (superposition). Qubits can become entangled, linking their states, and interference guides computations. This enables exploring many possibilities in parallel, offering a different way to solve certain problems.\n\n- Simple real-world example: finding a specific item in a huge database. A quantum computer could search more efficiently than a classical one, potentially reducing the effort from N checks to about sqrt(N) checks (Grover’s algorithm).\n\n- Why it’s important: for some problems, quantum computers could be dramatically faster, enabling breakthroughs in medicine, chemistry, optimization, cryptography, and AI-driven decision making. Today they’re experimental and best suited for specific tasks, not yet a universal replacement for ordinary computers."}
