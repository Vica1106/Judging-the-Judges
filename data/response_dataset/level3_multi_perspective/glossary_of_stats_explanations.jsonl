{"Major": "Statistics", "Term": "mutual independence", "Explanation": "Intuitive\nMutual independence means the occurrence of any event gives no information about any other; the probability of any combination equals the product of the individual probabilities. It is stronger than merely “not influencing each other.”\n\nFormal\nFor events A1,…,An, independence means: for every I ⊆ {1,…,n}, P(∩_{i∈I} A_i) = ∏_{i∈I} P(A_i). For random variables, the joint distribution factors: f(x1,…,xn) = ∏ f_i(x_i). Mutual independence implies pairwise independence, but the converse need not hold.\n\nPractical\n- Examples: three fair coin tosses; three fair dice.  \n- Underlies product-form likelihoods and the central limit theorem.  \n- In ML and statistics, independence of observations or features simplifies inference and modeling.\n\nBackground\nRelated concepts include pairwise independence, conditional independence, and the broader idea of product measures in probability theory.\n\nAnalogy\nLike independent ingredients in a recipe: one ingredient’s presence doesn’t alter the others, and the chance of a particular blend is the product of the individual chances."}
{"Major": "Statistics", "Term": "statistical inference", "Explanation": "Intuitive: You collect sample data (e.g., test scores, poll results) and use them as evidence about a population. Because the sample is imperfect, your conclusions are uncertain and must be stated probabilistically.\n\nFormal: Let θ be a population parameter. A statistic T(X) estimates θ; its sampling distribution governs accuracy. In frequentist inference, we build estimators, confidence intervals, and hypothesis tests (e.g., test H0: θ = θ0 with p-values). In Bayesian inference, we update a prior p(θ) with data to obtain a posterior p(θ|X).\n\nPractical: In medicine, A/B testing, or quality control, we estimate effects, decide if results are credible, and quantify uncertainty to guide decisions—often under model assumptions (independence, distributional form).\n\nBackground/related: Key ideas include sampling error, bias vs. variance, consistency, efficiency, and the central limit theorem. Distinguish estimation (quantifying θ) from decision making (tests, intervals). Frequentist and Bayesian viewpoints offer different ways to quantify uncertainty.\n\nAnalogy: Inference is like reconstructing a silhouette from a few shadows—the picture is uncertain, but with the right tools you can still learn meaningful features of the whole."}
{"Major": "Statistics", "Term": "joint distribution", "Explanation": "Intuitive: The joint distribution of two random variables X and Y tells you how their values occur together and how they depend on each other. It encodes all the information needed to describe their relationship.\n\nFormal: Discrete: joint PMF p(x,y)=P(X=x,Y=y), with sum p(x,y)=1. Continuous: joint PDF f_{X,Y}(x,y) with ∫∫ f=1; the joint CDF F_{X,Y}(x,y)=P(X≤x,Y≤y) also exists. Marginals: f_X(x)=∫ f_{X,Y}(x,y) dy, f_Y(y)=∫ f_{X,Y}(x,y) dx. Conditioning: f_{Y|X}(y|x)= f_{X,Y}(x,y)/f_X(x). Independence: f_{X,Y}(x,y)=f_X(x)f_Y(y).\n\nPractical: Lets you compute probabilities about (X,Y), make predictions P(Y|X), and estimate expectations or risks. Used in finance (joint returns), meteorology, genetics, Bayesian networks, etc.\n\nBackground: Related ideas include CDFs, correlation, and copulas that describe dependence structures; estimation can be parametric (Gaussian) or nonparametric.\n\nAnalogy: X and Y are dance partners; the joint distribution is the choreography that shows how they move together."}
{"Major": "Statistics", "Term": "random variable", "Explanation": "Intuitive: A random variable is a rule that assigns a single number to each outcome of a random experiment, turning uncertainty into numbers you can analyze.\n\nFormal: On a probability space (Ω, F, P), X: Ω→R is F-measurable; for every a, {ω: X(ω) ≤ a} ∈ F. Its distribution is P∘X^{-1}, with CDF F_X(x)=P(X≤x). If X takes countably many values, it is discrete with P(X=x_i)=p_i; otherwise it has density f with P(a≤X≤b)=∫_a^b f(x)dx.\n\nPractical: X could be the sum of dice, a waiting time, or a stock return; we use E[X], Var(X), and laws like the law of large numbers and the central limit theorem in forecasting, risk assessment, or simulations.\n\nBackground: Related ideas include random vectors, stochastic processes, moments and mgf; X is a function of outcomes, linking deterministic math with probabilistic behavior.\n\nAnalogy: A random variable is the numeric barcode you attach to each outcome, letting you read, compare, and predict the data."}
{"Major": "Statistics", "Term": "confidence interval (CI)", "Explanation": "(1) Intuitive: You estimate a parameter (e.g., a mean) and also report a plausible range that reflects uncertainty. If you repeated the study many times, about 1−α of the constructed intervals would contain the true value.\n\n(2) Formal: For parameter θ, a (1−α) confidence interval is an interval [L(X), U(X)] such that Pθ(L(X) ≤ θ ≤ U(X)) ≥ 1−α in repeated samples. It is derived from the sampling distribution (e.g., using a standard error). Example: x̄ ± t_{α/2,n−1} · s/√n (or x̄ ± z_{α/2} · σ/√n when σ is known).\n\n(3) Practical: In reports you often see “95% CI for the mean: [L, U]” computed from your data. Wider intervals reflect more variability or smaller samples; larger n generally produce narrower intervals. The interval conveys the precision of your estimate.\n\n(4) Background/related: Related to standard error and margin of error; a frequentist CI has a long-run coverage guarantee but does not assign a probability to θ itself. Bayesian credible intervals are a different, probabilistic statement about θ given the data.\n\nAnalogy: Think of a fishing net—if you used the same net many times, about 95% of the nets would catch the true fish population; your current net gives a plausible range where the fish are likely to be."}
{"Major": "Statistics", "Term": "covariance", "Explanation": "Intuitive: Covariance measures how two variables X and Y move together. If they rise together, Cov>0; if one tends to rise when the other falls, Cov<0; if there’s no linear relation, Cov ≈ 0.\n\nFormal: Cov(X,Y)=E[(X−μx)(Y−μy)]. For samples: Cov̂=(1/(n−1)) Σ (xi−x̄)(yi−ȳ). Properties: Cov(X,X)=Var(X); Cov(X,Y)=Cov(Y,X); Cov(aX+b,cY+d)=ac Cov(X,Y). Correlation ρ = Cov/(σx σy).\n\nPractical: Core in portfolio risk, PCA (covariance matrix), and regression intuition. If X,Y are independent, Cov=0. In regression, Cov informs the slope via Cov(X,Y)=Var(X)·β for simple linear fits.\n\nBackground: Cov is a fundamental link to variance and correlation, and contrasts with standardization (correlation). Estimation vs population issues matter; with normal data, zero covariance implies independence.\n\nAnalogy: Covariance is the dancers’ co-movement: positive means stepping in sync; negative, a counter-move; larger magnitude means tighter coordination."}
{"Major": "Statistics", "Term": "likelihood function", "Explanation": "Likelihood function L(θ; x) measures how plausible parameters θ are after observing data x. It is not P(θ|x) or P(x|θ) by itself; it is a function of θ given fixed data.\n\nFormal: If x1,...,xn have density f(x|θ) (iid), then L(θ; x) = ∏i f(xi|θ). The log-likelihood l(θ) = ∑i log f(xi|θ) is often used for stability. The goal is often to find θ that maximizes L (the maximum likelihood estimate, MLE).\n\nPractical: Widely used to estimate parameters (MLE), compare values via likelihood ratios, and choose models with AIC/BIC. In Bayesian work, the posterior is proportional to prior times likelihood. Real apps handle censored data, complex models, and rely on numerical optimization and profile likelihoods.\n\nBackground: Distinguish likelihood from probability; the likelihood principle treats data as fixed and varies θ. Related ideas include sufficiency, Fisher information, and the connection to hypothesis testing and model selection.\n\nAnalogy: Likelihood is the camera lens that shows which θ makes the observed data “look the most true.” The best focus (MLE) is where the image is sharpest."}
{"Major": "Statistics", "Term": "probability measure", "Explanation": "(1) Intuitive: A probability measure is a rule that assigns each event a number in [0,1], representing its likelihood. The whole space has probability 1, and for disjoint events A and B, P(A∪B)=P(A)+P(B). The rule is defined on a sigma-algebra—events closed under complements and countable unions.\n\n(2) Formal: Let Ω be the sample space and F a sigma-algebra of subsets of Ω. A function P:F→[0,1] is a probability measure if P(Ω)=1 and, for any countable collection of pairwise disjoint sets {A_i}⊂F, P(∪_i A_i)=∑_i P(A_i).\n\n(3) Practical: It describes distributions of random phenomena, underlies expectations E[X] and variances, and enables tools like Bayes’ rule P(A|B)=P(A∩B)/P(B) and the CDF F(x)=P(X≤x). It guides statistical modeling and inference.\n\n(4) Background/Related: Related to measures in measure theory; random variables are measurable maps; independence P(A∩B)=P(A)P(B); priors/posteriors are probability measures on hypotheses; connections to Lebesgue measure.\n\nAnalogy: A probability measure is a fixed budget allocated to all outcomes, additive across disjoint options, summing to 1."}
{"Major": "Statistics", "Term": "regression analysis", "Explanation": "- Intuitive: Regression seeks a simple rule linking an outcome Y to predictors X. It fits a line or surface so predictions are close to observed Y, showing how Y tends to change with X and enabling future predictions.\n\n- Formal: In linear regression, Y = β0 + β1X + ε with E[ε]=0, Var(ε)=σ^2. Parameters are estimated (usually by least squares) to minimize sum of squared residuals. For multiple regression, Y = β0 + β1X1 + … + βpXp + ε. Inference uses standard errors, t-tests, and R^2.\n\n- Practical: Used in economics, medicine, engineering, etc. Steps: specify the model, estimate parameters, diagnose residuals and fit (R^2), validate on new data, interpret coefficients, and make predictions.\n\n- Background: Distinguishes correlation from causation; beware overfitting. Extensions include regularized regression (Ridge/Lasso) and nonlinear/generalized linear models (e.g., logistic for binary outcomes).\n\n- Analogy: It’s like drawing the best-fitting path down a landscape to predict how the terrain (Y) changes as you move along a trail (X)."}
{"Major": "Statistics", "Term": "causal study", "Explanation": "- Intuitive: A causal study asks whether changing X would cause a change in Y, not just whether X and Y happen to move together.\n\n- Formal: Potential outcomes framework. For unit i, Y_i(1) and Y_i(0) are outcomes under treatment vs. control. Individual effect Δ_i = Y_i(1) − Y_i(0); average effect E[Δ] = E[Y(1) − Y(0)]. Identification relies on SUTVA (no interference, consistency), ignorability given covariates (unconfoundedness), and positivity. The do-operator do(X=x) formalizes an intervention: E[Y | do(X=x)].\n\n- Practical: Found in medicine (randomized trials), policy evaluation, economics. Methods include randomized experiments; observational approaches like propensity scores, instrumental variables, regression discontinuity, and difference-in-differences to estimate causal effects from data.\n\n- Background: Distinguish correlation from causation; confounding concerns; causal graphs (DAGs); Rubin Causal Model; Pearl’s do-calculus.\n\n- Analogy: It’s like testing a recipe by changing one ingredient and seeing if the dish’s outcome changes, while keeping other factors constant."}
