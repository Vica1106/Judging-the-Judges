{"Major": "Computer Science", "Term": "quantum computing", "Explanation": "- Intuitive perspective:\nA quantum computer uses qubits that can be 0, 1, or both at once. Through entanglement and interference, many possibilities are explored in parallel, and the correct outcome emerges when amplitudes interfere constructively upon measurement.\n\n- Formal perspective:\nA qubit is a unit vector in C^2; n qubits live in (C^2)^{⊗ n}. Computations are unitary evolutions U, followed by measurement in the computational basis. The quantum circuit model uses gates (e.g., Hadamard, CNOT, T). Outcomes follow the Born rule; the complexity class BQP captures efficiently solvable problems.\n\n- Practical perspective:\nToday’s devices are in the NISQ era: superconducting qubits and trapped ions implement small quantum circuits. Promising applications include factoring (Shor), search (Grover), and simulating quantum systems; real-world use often targets chemistry or optimization. Challenges include decoherence, gate errors, and the need for fault-tolerant error correction.\n\n- Background concepts:\nKey ideas include superposition, entanglement, interference; no-cloning; measurement postulate; decoherence; quantum error correction codes and fault tolerance.\n\n- Analogy:\nThink of a choir trying many harmonies at once; a single measured note reveals a useful tune derived from the ensemble."}
{"Major": "Computer Science", "Term": "big O notation", "Explanation": "Big O is a way to describe how a program’s running time or memory use grows with input size n.\n\n(1) Intuitive: It captures growth rate and scalability while ignoring constants and small terms. It tells you how performance behaves as problems get large.\n\n(2) Formal: f(n) = O(g(n)) means there exist constants c > 0 and n0 such that for all n ≥ n0, f(n) ≤ c·g(n). It’s an upper bound on growth (often for worst-case time or space).\n\n(3) Practical: Used to compare algorithms and pick data structures. It helps set expectations for scalability: e.g., O(log n) (binary search) grows slowly, O(n) (linear scan) grows linearly, O(n log n), O(n^2) (quadratic) for larger n.\n\n(4) Background: Related notions include Theta (tight bound) and Omega (lower bound). Amortized vs worst-case and average-case matters. Big-O ignores constants and lower-order terms, yet real-world measurements still depend on hardware and constants.\n\nAnalogy: Big O is a growth-rate speed limit: it says the algorithm won’t outpace g(n) (up to a constant factor) as n grows."}
{"Major": "Computer Science", "Term": "semantics", "Explanation": "Semantics in computer science is the study of meaning: what programs and language constructs actually do, beyond their syntax.\n\n- Intuitive perspective: It answers the question, “what effect does this code have on the machine state?” For example, x := x+1 increases x by one.\n\n- Formal perspective: Several precise approaches exist:\n  - Denotational semantics maps programs to mathematical objects.\n  - Operational semantics describes step-by-step execution rules.\n  - Axiomatic semantics (e.g., Hoare logic) uses pre/post-conditions to specify and reason about correctness.\n\n- Practical perspective: Semantics guides compilers/ interpreters to preserve behavior during optimization, supports program verification, informs language design and API contracts, and underpins data schemas and interoperability.\n\n- Background/related concepts: Syntax vs semantics; typing and binding; lambda calculus and domain theory; formal verification; NLP semantics and pragmatics; ontologies and schema semantics.\n\nAnalogy: Semantics are the recipe’s meaning and effects—the dish’s taste—while syntax is the ingredients and steps; together they define what the code is and what it does."}
{"Major": "Computer Science", "Term": "floating-point arithmetic", "Explanation": "Intuitive: Floating-point numbers are a finite, approximate way to represent real numbers as sign × mantissa × base^exponent. In binary, values are stored as sign, exponent, and significand, with normalization to [1,2). This yields a dense set of near-real values but with gaps between representable numbers.\n\nFormal: In IEEE 754 binary64, a nonzero number is ±(1.fraction) × 2^(exponent − bias). There are special values (0, subnormals, ∞, NaN). Arithmetic is performed with infinite precision and then rounded to nearest (ties to even). Error is described by ulp and machine epsilon.\n\nPractical: Floating-point underpins almost all numeric code (graphics, simulations, ML). Issues include rounding error, overflow/underflow, and cancellation. Good practice: pick appropriate precision, use compensated summation, scale data, and rely on robust libraries; be mindful of propagation of NaN and infinities.\n\nBackground: Compared with fixed-point, floating-point trades exactness for a broad range and dynamic scaling. Key ideas include normalization, subnormal numbers, rounding modes, and error analysis for stability.\n\nAnalogy: It’s like a ruler with fixed ticks—every measurement and calculation lands on the nearest tick, sometimes losing fine details, especially when mixing very different scales."}
{"Major": "Computer Science", "Term": "quicksort", "Explanation": "Intuitive:\nQuicksort picks a pivot and partitions the list into elements less than, equal to, and greater than the pivot, then recursively sorts the sublists. It’s a divide-and-conquer, in-place approach that rearranges elements toward order with minimal extra memory.\n\nFormal:\nFor n elements, choose pivot p and partition into L (< p), E (= p), G (> p). Recursively sort L and G; base case n ≤ 1. If T(n) = T(k) + T(n−k−1) + Θ(n), then average time is Θ(n log n); worst-case Θ(n^2). In-place variants exist (Lomuto/Hoare); stability is not guaranteed.\n\nPractical:\nWidely used in libraries (e.g., Java’s dual-pivot quicksort, C/C++ std::sort). Pivot strategy (random, median-of-three) reduces worst-case risk; often paired with insertion sort for small subarrays and other hybrid optimizations for performance.\n\nBackground/related:\nIllustrates core ideas of partitioning, divide-and-conquer, and recursion. Compared to merge sort (stable, extra space) and impacted by data locality and pivot choice.\n\nAnalogy:\nPivot is a compass; partition draws two routes; recursion stitches the routes into a single, ordered map."}
{"Major": "Computer Science", "Term": "agent-based model (ABM)", "Explanation": "- Intuitive perspective: ABMs model systems as many autonomous agents with simple local rules in a shared environment; global patterns arise from their interactions, not central control. Example: pedestrians avoiding collisions.\n\n- Formal perspective: Definition: a multi-agent system with agents i ∈ A, each with state s_i, local rules r_i, and actions a_i; an environment E. Dynamics are given by a transition function T updating {s_i}, E over time (discrete or continuous), often with stochastic elements and local perception.\n\n- Practical perspective: Used in traffic, epidemiology, crowd dynamics, economics, ecology. Implementations include NetLogo, Mesa, Repast. Process: specify agents/environment, define local rules and topology, initialize, run simulations, analyze emergent outcomes; calibrate to data and perform sensitivity analyses.\n\n- Background and related concepts: Related to cellular automata and complex/adaptive systems; ABMs emphasize bottom-up emergence, heterogeneity, and adaptation; can incorporate learning or decision-making.\n\nAnalogy: Think of a beehive—many simple bees following local rules create complex colony behavior."}
{"Major": "Computer Science", "Term": "big data", "Explanation": "Big data is the term for data sets so large and complex that traditional tools can’t manage them efficiently.\n\n- Intuitive: It’s data that’s enormous in size, generated rapidly, and diverse in type, yet contains patterns visible only when you examine the whole.\n\n- Formal: It denotes data whose volume, velocity, and variety exceed conventional DBMS and analytics; requires distributed storage/compute and pipelines to extract value.\n\n- Practical: You see it in web logs, social feeds, IoT sensors, genomics, and fraud detection—where batch and streaming processing, data lakes, and ML pipelines turn raw data into insights.\n\n- Background: Related ideas include data mining, ML, data governance, privacy, NoSQL vs SQL, and the trade-offs between latency, throughput, and consistency in distributed systems.\n\nAnalogy: It’s like trying to read an entire library with a single pencil—many readers, many tools, and smart methods are needed to uncover the meaningful stories."}
{"Major": "Computer Science", "Term": "class", "Explanation": "Intuitive: A class is a blueprint for objects. It describes the data an object holds (fields) and the actions it can perform (methods). You create objects from the same class, each with its own state but the same structure and behavior.\n\nFormal: A class defines a user-defined type. It specifies a set of fields and member functions (often including constructors). Through inheritance a class can extend another, forming a subtype relation; interfaces/abstract classes express contracts. Generics/parametric types let a class be reused with different data types.\n\nPractical: Classes organize code in object-oriented languages (Java, C++, Python). They enable encapsulation, reuse, polymorphism, and design patterns; you model real entities (User, Window) or abstractions (Collection) as classes.\n\nBackground: Related concepts include objects vs classes vs instances, encapsulation, inheritance, polymorphism, composition, and interfaces; metaclasses in dynamic languages.\n\nAnalogy: A class is a blueprint for a family of objects—like a cookie-cutter for cookies: many cookies share shape and features but differ in size or flavor."}
{"Major": "Computer Science", "Term": "coding theory", "Explanation": "Intuitive: Coding theory asks how to send messages reliably over noisy channels by adding structured redundancy—like proofreading a note. A code maps messages to codewords; the distance between codewords reflects how many errors can be corrected.\n\nFormal: A code C is a subset of A^n with minimum distance d. Encoding E: M -> C; decoding recovers M from a possibly corrupted y ∈ A^n. For linear codes, C is a subspace with generator and parity-check matrices. Key parameters: length n, size |C|, rate R = log|C|/n, minimum distance d. Error detection/correction depends on d. Channel models (BSC, AWGN) and Shannon’s theorems define performance limits.\n\nPractical: Used in data storage (CD/DVD/HDD), QR codes, and wireless networks (Wi‑Fi, 4G/5G); memory ECC, RAID, and data-integrity protocols rely on codes and checksums.\n\nBackground: Ties to information theory, combinatorics, and algebra; core ideas include Hamming distance, linear vs nonlinear codes, redundancy–reliability trade-offs, and capacity versus achievable rate.\n\nAnalogy: Encoding adds padding that survives noise; decoding is a detective reconstructing the original note from smeared letters."}
{"Major": "Computer Science", "Term": "computability theory", "Explanation": "Computability theory asks which problems can be solved by an algorithm, in principle, regardless of time or space.\n\nIntuitive perspective:\n- Some tasks admit a clear, repeatable procedure; others do not. The Halting problem shows there is no general algorithm to decide, for every program and input, whether it halts.\n\nFormal perspective:\n- A function f: N^k → N is computable if a Turing machine halts on every input and outputs f(x). A decision problem has a computable characteristic function. A problem is decidable if it is computable; semi-decidable (r.e.) if a TM halts and accepts yes-instances, but may loop on no-instances. Reducibility A ≤_m B lets us solve A using a solver for B. The Church–Turing thesis posits that any effectively computable function is computable by a Turing machine (or equivalent model).\n\nPractical perspective:\n- In practice we use restricted models and tools (finite automata, RAM models, model checking, SMT solvers) and rely on heuristics, restricted logics, or approximations because many questions are undecidable in general.\n\nBackground and related concepts:\n- Connections to recursion theory, lambda calculus, Gödel’s incompleteness, and complexity theory; notions of decidability, reducibility, completeness, and Kolmogorov complexity.\n\nAnalogy: computability maps the boundary between what can be solved by a single blueprint in principle and what remains permanently unreachable."}
