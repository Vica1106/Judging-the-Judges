{"Major": "Artificial Intelligence", "Term": "Selective Linear Definite Clause Resolution", "Explanation": "Intuitive\nThink of a knowledge base as rules of the form Head :- Body1,...,BodyN. To prove a query, pick one subgoal (a literal), use a rule whose head matches it, replace the subgoal by the rule’s body, and repeat. This yields a single, linear trace through the rules, guided by which subgoal you resolve next.\n\nFormal\nP is a set of definite clauses. A goal G is a finite sequence of atoms. An SLD-derivation is a sequence G0 => G1 => ... => Gk where each step selects an atom A in Gi, unifies A with a clause H <- B1,...,Bk via θ = mgu(A,H), and forms Gi+1 = (Gi \\ {A}) ∪ {B1,...,Bk}θ. Success means Gn is empty; failure when no step applies. “Linear” means one selected atom is resolved per step; “selective” means a selection rule (often the leftmost) chooses A.\n\nPractical\nUnderlies query answering in Prolog and many AI systems: recursive definitions, database lookups, and rule-based reasoning. Efficiency and termination depend on indexing, control strategies, and possible non-termination.\n\nBackground\nRelated: Horn clauses, unification, resolution, SLD trees, and negation as failure (SLDNF); core concepts in logic programming and automated reasoning.\n\nAnalogy\nChoosing one thread in a threadbare rope and pulling it through a sequence of knots—one knot at a time—to reveal the answer."}
{"Major": "Artificial Intelligence", "Term": "Big O notation", "Explanation": "- Intuitive: Big O describes how running time or memory grows with input size n. It ignores constants and small terms and highlights the dominant growth.\n\n- Formal: f(n) = O(g(n)) means there exist constants c > 0 and n0 such that for all n ≥ n0, f(n) ≤ c · g(n). It bounds the worst-case growth as n grows large.\n\n- Practical AI perspective: Helps compare models and data pipelines. Example: training time often scales as O(NP) (N = samples, P = parameters). Transformer attention can scale as O(L^2 d) with sequence length L and dimension d. Inference latency and memory depend on hardware, but asymptotic growth guides design choices and tradeoffs (e.g., faster-but-greedy approximations).\n\n- Background concepts: Related ideas include Theta (tight bound) and Omega (lower bound); amortized and average-case analyses; Big-O vs exact timing and empirical measurements.\n\n- Analogy: Big O is a growth forecast—an upper bound you plan for as data or model size increases, not the exact bill today."}
{"Major": "Artificial Intelligence", "Term": "neural machine translation (NMT)", "Explanation": "1) Intuitive perspective\nNMT treats translation as a single neural function: it reads a source sentence and, using global context, writes a fluent target sentence. An encoder compresses X; a decoder, guided by attention, produces Y.\n\n2) Formal perspective\nWe model p(Y|X;θ) with a seq2seq network: X=(x1,...,xT), Y=(y1,...,yS). Train to maximize log-likelihood on parallel data; architectures include Transformer or RNNs with attention; tokens use subword units (BPE); decoding uses beam search.\n\n3) Practical perspective\nIn practice, NMT powers apps like Google/DeepL Translate, enabling many languages and often real-time use. It runs on GPUs/TPUs, relies on parallel corpora and domain adaptation, and uses subword tokenization. Evaluation uses BLEU; challenges include rare words and domain shift.\n\n4) Background/related concepts\nRooted in the encoder–decoder with attention; Transformer popularized it; it improves over phrase-based SMT and connects to multilingual NMT, transfer learning, and contextualized embeddings.\n\nAnalogy: like a fluent polyglot interpreter who reads an entire paragraph and faithfully rewrites it with context."}
{"Major": "Artificial Intelligence", "Term": "NP-hardness", "Explanation": "- Intuitive: NP-hard means as hard as the hardest problems in NP. If you could solve an NP-hard problem in polynomial time, you could solve every NP problem in polynomial time via reductions.\n\n- Formal: A problem H is NP-hard if for every L in NP, L ≤_p H (polynomial-time many-one reduction). If H ∈ NP as well, H is NP-complete. This framing applies to decision problems; for optimization tasks, the associated decision version is typically used to discuss hardness.\n\n- Practical in AI: Many AI tasks are NP-hard or NP-complete (e.g., SAT, scheduling, graph coloring, vehicle routing). In practice, we use heuristics, incomplete or approximate algorithms, or exploit special-case structure; exact polynomial-time solutions are unlikely unless P=NP.\n\n- Background/related concepts: Key ideas include reductions, the Cook–Levin theorem, NP, NP-complete, P vs NP, and approaches like approximation, fixed-parameter tractability, and SAT-based methods.\n\n- Analogy: NP-hard is the “hardest puzzle” in the NP puzzle book; solving any one quickly would unlock all others."}
{"Major": "Artificial Intelligence", "Term": "true quantified Boolean formula", "Explanation": "- Intuitive perspective: A true quantified Boolean formula (TQBF) is a game: players alternate choosing truth values for variables according to a prefix of exists and for all. The formula is true if the existential player has a strategy to make the matrix true no matter how the universal player moves.\n\n- Formal perspective: A QBF has the form Q1 x1 Q2 x2 ... Qn xn · φ(x1,...,xn), where each Qi ∈ {∃, ∀} and φ is a boolean matrix. The closed formula is true under standard semantics if the quantified evaluation yields true. The decision problem TQBF asks whether a given closed QBF is true; it is PSPACE-complete.\n\n- Practical perspective: TQBF appears in formal verification, model checking, and synthesis (e.g., hardware verification, controller synthesis, planning under adversarial environments). QBF encodings capture statements about all possible environments or all possible inputs and require strategies, not just single assignments.\n\n- Background/related concepts: Related to SAT but with alternation (existential vs universal). Semantics connect to game theory; prenex form, Skolemization, and the PSPACE complexity reflect the difficulty of reasoning with alternating quantifiers; the “matrix” is the core propositional part.\n\nAnalogy: It’s a two-player game where the existential player must secure a win against all moves of the universal player. If they can, the formula is true."}
{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "- Intuition: Algorithmic probability weights outputs by how short a program can produce them on a universal computer; simple, compressible patterns are more probable than complex ones.\n\n- Formal: Let U be a fixed universal Turing machine. For string x, M(x) = sum_{p: U(p)=x} 2^{-|p|}, where p are self-delimiting programs. M is Solomonoff’s universal prior; K(x) ≈ -log2 M(x). The measure is incomputable.\n\n- Practical: Not computable in general, but serves as an ideal for sequence prediction and model selection. Real work uses approximations (MDL, Bayesian mixtures, bounded priors) and compression-based learning.\n\n- Background: Related to Kolmogorov complexity K(x), Occam’s razor, MDL, and Solomonoff induction; connects complexity, probability, and inference.\n\n- Analogy: If a chorus favors the simplest tunes, the most succinct blueprints—short programs—sound the loudest, guiding predictions across intuition, formalism, and practice."}
{"Major": "Artificial Intelligence", "Term": "behavior informatics (BI)", "Explanation": "- Intuitive perspective: Behavior informatics (BI) studies how information, context, and stimuli shape human actions, turning those actions into data to understand and guide behavior.\n\n- Formal perspective: BI is an interdisciplinary framework uniting information science, cognitive/social science, and computing. A formal view models humans H, actions A, information I, context C, and observations O over time T. A behavior model M: (I, C, history) → A is learned from data D. Evaluation uses predictive accuracy, causal validity, and intervention impact, with tools from time-series, probabilistic graphs, and machine learning.\n\n- Practical perspective: BI appears in UX analytics, recommender systems, health-behavior apps, marketing, smart cities, education, and policy design—where logs, sensors, and social data underpin models of what people do, why, and how to influence it, all with attention to privacy.\n\n- Background and related concepts: intersects behavioral data science, HCI, affective computing, causal inference, and ethics; complements AI by providing interpretable, behaviorally informed priors and models.\n\nAnalogy: BI is like weather forecasting for human activity—gather signals, build models, predict outcomes, and guide choices amid uncertainty."}
{"Major": "Artificial Intelligence", "Term": "big data", "Explanation": "(1) Intuitive: Big data means data sets so large and fast-moving that traditional databases and analysis tools struggle to capture, store, or extract insights. When you combine many sources (logs, text, images, sensor streams) you can reveal patterns impossible from small samples.\n\n(2) Formal: Defined by the 4 Vs: Volume, Velocity, Variety, Veracity (and often Value). Data are stored and processed with distributed systems (e.g., Hadoop, Spark) and scalable analytics, not on a single machine, enabling models that grow with data size.\n\n(3) Practical: In real apps, big data appears in web logs, social-media streams, IoT sensor networks, genomics, finance. Used for predictive modeling, personalization, fraud detection, and real-time monitoring. Challenges include data quality, privacy, governance, and the need for data lakes and streaming pipelines.\n\n(4) Background/related concepts: data science, data mining, machine learning, ETL, data governance, NoSQL, streaming vs. batch processing.\n\nAnalogy: It’s like turning a firehose of information into a scalable, real-time map that guides decisions."}
{"Major": "Artificial Intelligence", "Term": "convolutional neural network", "Explanation": "- Intuitive perspective: A CNN processes grid-like data (e.g., images) by sliding a small filter over the input to detect simple patterns (edges, textures). The same filter is applied everywhere, so the network recognizes features anywhere and builds a hierarchy of increasingly complex representations.\n\n- Formal perspective: The core operation is convolution. For each filter W and bias b, the output at position (i,j) is O(i,j) = σ( sum_{u,v} W(u,v) X(i+u, j+v) + b ), with stride s and padding p. Multiple filters yield multiple feature maps; parameters ≈ (#filters)×(filter height×filter width) + biases. Backprop updates W during training.\n\n- Practical perspective: Common blocks are conv → activation → pooling, stacked to form deep architectures (e.g., ResNet, VGG). They excel at image classification, object detection, segmentation, video analysis, and even processing spectrograms in audio tasks, thanks to parameter efficiency and translation invariance.\n\n- Background/Related concepts: Key ideas include padding, stride, pooling, dilation, receptive field, and transfer learning. CNNs leverage local connectivity and weight sharing to learn hierarchical, spatially aware features.\n\n- Analogy: Think of sliding a reusable stencil across a picture—the stencil finds the same motif anywhere and, by stacking motifs, reveals the whole scene."}
{"Major": "Artificial Intelligence", "Term": "Darkforest", "Explanation": "Dark Forest theory: In a vast cosmos, civilizations stay silent. Any beacon could reveal your location to a predator; contact can invite destruction, so hiding becomes optimal. The universe feels like a dark forest of unseen actors.\n\nFormal: Definition: A Dark Forest is a game with N rational agents choosing signaling a_i∈[0,1]. Detection prob D(a) rises with a; payoff U_i = B_i·P(contact) − R_i·P(detection). If any rival may be hostile, the (risk-ddominant) equilibrium is a_i*=0 (silence).\n\nPractical: In AI safety and cybersecurity, we limit disclosure, minimize attack surfaces, and favor privacy-preserving designs; in SETI/space policy, the debate weighs active signaling against listening and restraint; in distributed systems, signaling risk shapes protocol transparency.\n\nBackground: Related ideas include the Fermi paradox, signaling games, and game-theoretic security; key terms: opacity, asymmetric information.\n\nAnalogy: It’s like a hunter’s night forest: stay quiet and hidden, because any spark could reveal you to danger."}
