{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "Intuitive: Think of predicting data by considering all possible computer programs that could generate it. A random program is likelier to be short than long, so simpler generating explanations dominate. The data you see is the output of some program; algorithmic probability measures how likely that output is under all programs.\n\nFormal: Pick a fixed universal prefix-free Turing machine U. For any string x, define m(x) = sum_{p: U(p)=x} 2^{-|p|}, where |p| is the length of program p. This m is the universal a priori probability. It relates to Kolmogorov complexity K(x) = min_p{|p|: U(p)=x} by m(x) ≈ 2^{-K(x)} up to a constant. The measure is noncomputable (halting problem) but serves as a theoretical baseline.\n\nPractical: In AI, it underpins ideas in induction, model selection, and data compression (MDL) via preferring simpler explanations. Although not computable, it guides approximations and Bayesian model averaging with short description length.\n\nBackground: Ties to Occam’s razor, Kolmogorov complexity, MDL, universal priors; related to Bayesian inference as model-averaging over program-generated data.\n\nAnalogy: A library of all possible recipes; a short recipe is more probable to produce your dish, so the simplest explanations win."}
{"Major": "Artificial Intelligence", "Term": "attributional calculus", "Explanation": "Intuitive: Attributional calculus is a framework for attributing a model’s prediction to its input features using a coherent set of rules that specify how attributions combine as signals flow through the model.\n\nFormal: For a predictor f: R^n → R and a baseline x0, a local attribution φ ∈ R^n satisfies ∑ φ_i = f(x) − f(x0) (local accuracy). When f is built as a composition f = f_m ∘ ... ∘ f_1, the calculus prescribes how to propagate attributions through the layers (e.g., chain-rule gradients, integrated gradients, or SHAP-style decompositions).\n\nPractical: Used to explain predictions, diagnose model behavior, and audit fairness in domains like medicine, finance, and computer vision; helps identify which features or regions drive a decision.\n\nBackground: Related to feature importance, Shapley values, and sensitivity analysis; connected to methods like LRP and DeepLIFT; grounded in ideas from cooperative game theory and numerical analysis.\n\nAnalogy: It’s like a recipe that shows how much each ingredient—and each mixing step—contributes to the final dish, so you can trace flavor from start to finish."}
{"Major": "Artificial Intelligence", "Term": "statistical relational learning (SRL)", "Explanation": "- Intuitive: SRL models data with many objects and relations under uncertainty. It uses relational structure (who is related to whom) to share information and reason about attributes and links.\n\n- Formal: A probabilistic relational model defines a distribution over interpretations of a relational vocabulary. Markov Logic Networks (MLNs) are canonical: a set of first-order formulas with weights; P(world) ∝ exp(Σ w_i · N_i(world)), where N_i is how many groundings of formula i hold. Learning estimates weights from data; inference computes P(query | evidence).\n\n- Practical: used for link prediction in knowledge graphs and social networks, relation extraction in NLP, protein interactions in biology, fraud detection, and recommender systems—anywhere relations matter for predicting attributes or missing links.\n\n- Background: bridges first-order logic and graphical models; related SRL families include PRMs and PSL. Key challenges: scalability, incomplete data; approaches like lifted inference exploit symmetries to speed up computation.\n\n- Analogy: SRL is a map where roads define how places influence each other; you update beliefs about attributes and links as you observe more traffic."}
{"Major": "Artificial Intelligence", "Term": "metabolic network reconstruction and simulation", "Explanation": "1) Intuitive perspective\nMetabolic network reconstruction maps all chemical reactions in a cell and then simulates how fast they run under resource limits to predict what the cell can produce or consume.\n\n2) Formal perspective\nFormally, a network is a stoichiometric matrix S (metabolites × reactions). Under steady state Sv = 0 with bounds l ≤ v ≤ u, an objective c^T v is optimized (flux balance analysis). Dynamic or thermodynamic constraints extend this framework.\n\n3) Practical perspective\nIn practice, genome-scale models (GEMs) for organisms like E. coli or yeast are built and used to predict gene knockouts, guide metabolic engineering, or infer disease mechanisms. Tools include COBRA Toolbox, cobrapy; data from genomics, transcriptomics, and metabolomics refine reconstructions.\n\n4) Background perspective\nThis sits at the intersection of graph theory, linear programming, gap filling, and omics integration—concepts like flux variability, elementary flux modes, and constraint-based modeling deepen understanding.\n\nAnalogy\nThink of it as building a city map of roads and then running traffic simulations to see flows under constraints."}
{"Major": "Artificial Intelligence", "Term": "dynamic epistemic logic (DEL)", "Explanation": "DEL (Dynamic Epistemic Logic) studies how knowledge and beliefs of multiple agents change when actions occur or information is shared.\n\nIntuition: knowledge is updated by announcements, observations, or messages; DEL models how each agent’s possibilities shrink or shift after events and how this propagates to others.\n\nFormal: Base language includes K_i φ (agent i knows φ). A Kripke model M = (W, {R_i}_{i∈A}, V) has worlds W, epistemic relations R_i ⊆ W×W, and valuation V. An action model A = (E, {R_i^A}_{i∈A}, pre, post) encodes possible events, preconditions, and how propositions may change. The product update M ⊗ A = (W×E, {~_i}, V') yields a new model; (w,e) ~_i (w',e') iff w ~_i w' and e ~_i^A e'. Truth is defined by M,w ⊨ [A,e] φ iff (M ⊗ A, (w,e)) ⊨ φ. Public announcements are a special case with E={e}.\n\nPractical: used in AI for planning under epistemic uncertainty, multi-agent systems, security protocol verification, dialogue and negotiation, and game-theoretic reasoning.\n\nBackground: relates to epistemic logic, S5, common knowledge, PAL, and product updates.\n\nAnalogy: like updating a shared map as new information arrives; everyone’s view of the terrain changes, and those changes ripple through the group."}
{"Major": "Artificial Intelligence", "Term": "neural Turing machine (NTM)", "Explanation": "Intuition\nA neural network controller plus an external, differentiable memory bank that it can read from and write to with soft attention, enabling it to learn and execute algorithms (copying, sorting) by manipulating memory like a working tape.\n\nFormal\nAn NTM consists of a controller f with state h_t and an external memory M_t ∈ R^{N×W}. At each step t it emits an interface θ_t that parameterizes read/write heads. Read: r_t = M_t w^r_t, where w^r_t is a differentiable weighting over slots via content-based addressing κ_t = softmax(M_t k_t) and a shift/sharpening mechanism. Write: M_{t+1} = M_t ⊙ (1 - w^w_t e_t^T) + w^w_t a_t^T, with erase e_t ∈ [0,1]^W and add a_t ∈ R^W. The system is trained end-to-end by backpropagation through time.\n\nPractical\nNTMs demonstrate learned algorithms on synthetic tasks (copy, reverse, sort) and inspired memory-augmented architectures (DNC) used for reasoning, program induction, and QA requiring long-range memory.\n\nBackground\nRelated to Turing machines, differentiable RAM, and attention mechanisms; bridges neural networks with algorithmic, symbolic-style computation.\n\nAnalogy\nLike a smart librarian robot that uses an adjustable, indexable shelf to fetch and place notes as it reads."}
{"Major": "Artificial Intelligence", "Term": "answer set programming (ASP)", "Explanation": "(1) Intuitive: Answer set programming (ASP) lets you describe a problem with rules and constraints, not steps to follow. Solutions are answer sets—self-consistent collections of facts that satisfy all rules, including default assumptions expressed via negation as failure.\n\n(2) Formal: A program is a finite set of rules: head ← body1, ..., bodyn, not neg1, ..., not negm. Under answer-set (stable-model) semantics, an answer set is a minimal, self-justifying model obtained (via the Gelfond–Lifschitz reduct) after grounding the program into a propositional form. Each answer set corresponds to a solution.\n\n(3) Practical: Used in configuration, scheduling, planning, and knowledge representation. Models with predicates and constraints are fed to solvers (e.g., Clingo, DLV) that compute all feasible answer sets or optimal ones, enabling declarative modeling of complex combinatorial problems.\n\n(4) Background/related: Related to nonmonotonic reasoning, default logic, circumscription, and to SAT/constraint programming. Pros: high-level problem descriptions and portable semantics; cons: grounding blow-up and solver performance trade-offs.\n\nAnalogy: ASP is a rule-based map; the solver scans it to find every region (answer set) that satisfies all roads (rules) and avoids dead ends (negation as failure)."}
{"Major": "Artificial Intelligence", "Term": "NP-completeness", "Explanation": "Intuitive: NP problems are those where a proposed solution can be verified quickly. NP-complete problems are the hardest in NP: if you could solve one NP-complete problem in polynomial time, you could solve every NP problem in polynomial time (so P=NP would hold).\n\nFormal: NP = languages decidable by a nondeterministic Turing machine in poly time, equivalently problems with polynomial-time verifiable certificates. A problem is NP-complete if (i) it lies in NP, and (ii) every problem in NP reduces to it via a polynomial-time many-one reduction. Cook–Levin showed SAT is NP-complete; Karp’s list popularized many others (3-SAT, CLIQUE, Vertex Cover, Hamiltonian Path).\n\nPractical: Many real-world AI and CS tasks are NP-hard (scheduling, routing, planning, resource allocation). We use SAT solvers, constraint programming, heuristics, approximations, or fixed-parameter tractability to handle large instances, accepting practical speedups rather than worst-case guarantees.\n\nBackground: Related notions include NP-hard (at least as hard as NP problems) and co-NP; reductions formalize “no easy shortcut.” The P vs NP question remains unresolved.\n\nAnalogy: NP-complete problems are the bottleneck puzzles in a class—crack one quickly, and you unlock quick solutions for all; fail, and you stay stuck."}
{"Major": "Artificial Intelligence", "Term": "partially observable Markov decision process (POMDP)", "Explanation": "- Intuitive perspective: When the true state is hidden, you maintain a belief (a probability distribution over states), update it with noisy observations, and choose actions to maximize expected reward despite uncertainty.\n\n- Formal perspective: A POMDP is the tuple (S, A, O, T, Z, R, γ). S: states; A: actions; O: observations; T(s'|s,a): state transition; Z(o|s',a): observation model; R(s,a): reward; γ: discount. Belief b ∈ Δ(S) summarizes knowledge. Belief update: b'(s') ∝ Z(o|s',a) ∑_s T(s'|s,a) b(s). Policy π: belief→action; value V(b) via Bellman equations on the continuous belief space. Exact solutions are intractable; practical methods are approximate (point-based, POMCP, etc.).\n\n- Practical perspective: Used in robotics, autonomous vehicles, dialogue systems, and medical decision making—any domain with noisy sensors, occlusions, or delayed feedback, where the agent must plan under uncertainty.\n\n- Background/related concepts: Extends MDPs to partial observability; related to Bayesian filtering and hidden Markov models; belief-space planning trades exactness for tractability via approximations.\n\n- Analogy: POMDPs are like navigating fog with a probabilistic map—you never see the exact landscape, but you continuously update your belief and decide steps to reach your goal safely."}
{"Major": "Artificial Intelligence", "Term": "quantum computing", "Explanation": "- Intuitive perspective: A qubit can be 0 and 1 at once; many qubits can be entangled, so their states are correlated. Quantum interference lets a circuit explore many possibilities and amplify the chance of the correct answer.\n\n- Formal perspective: The system lives in a Hilbert space; gates implement unitary transformations; measurement yields classical outcomes with probabilities given by amplitudes. Computation is a quantum circuit; complexity class BQP captures efficiently solvable problems. Core ideas include superposition, entanglement, tensor products, amplitude amplification, and the quantum Fourier transform; no-cloning and error correction are essential.\n\n- Practical perspective: Today’s devices are NISQ—tens to a few hundred noisy qubits (superconducting, trapped ions, etc.). No full fault tolerance yet. Near-term work uses variational algorithms (VQE, QAOA) and quantum simulators for chemistry/materials. Potential AI angles include faster sampling, optimization, and certain ML tasks, but quantum advantage is problem- and hardware-dependent.\n\n- Background and related concepts: Requires linear algebra, probability, and information theory. Key ideas: decoherence, quantum error correction, and the distinction between quantum and classical resources. Related topics: quantum annealing, hybrid quantum-classical workflows.\n\nAnalogy: a multi-shelf library read by a magic lens that boosts the right passage but needs careful pruning of noise."}
