{"Major": "Statistics", "Term": "mutual independence", "Explanation": "Mutual independence (statistics)\n\n- Basic meaning: A set of events A1, A2, …, An are mutually independent if, for every subset of these events, the probability of all of them happening equals the product of their individual probabilities. In symbols: P(Ai1 ∩ Ai2 ∩ … ∩ Aik) = ∏ P(Aij) for any subset. This is stronger than just pairwise independence (two at a time).\n\n- Simple real-world example: Three fair coin flips. Let A1 = “flip 1 is heads,” A2 = “flip 2 is heads,” A3 = “flip 3 is heads.” Then P(A1 ∩ A2 ∩ A3) = (1/2)^3 = 1/8, and for any subset, P(Ai ∩ Aj) = (1/2)^2, etc. The outcome of one flip doesn’t affect the others.\n\n- Why it’s important: It lets us multiply probabilities to find chances of multiple events at once, simplifying analysis and modeling. If events aren’t independent, multiplying probabilities can give wrong results, leading to incorrect conclusions in experiments and data analysis."}
{"Major": "Statistics", "Term": "statistical inference", "Explanation": "Statistical inference is the process of using data from a small group (a sample) to draw conclusions about a larger group (the population). It combines estimates with a measure of uncertainty, using probability.\n\nReal-world example: You want to know the average height of adult residents in a city. You measure 200 randomly chosen people. From this sample, you estimate the city’s average height and report a margin of error (e.g., plus/minus 2 cm). You might also test whether the average height is greater than a threshold (hypothesis testing).\n\nWhy it’s important: In many cases we can’t measure everyone, so inference lets us learn about big groups without complete data. It guides decisions in medicine, policy, business, and science, while explicitly acknowledging uncertainty so we don’t overstate what the data show."}
{"Major": "Statistics", "Term": "joint distribution", "Explanation": "Joint distribution\n\n(1) Basic meaning: It describes how two (or more) random variables behave together. It assigns a probability to every possible combination of outcomes. For discrete variables, P(X=x, Y=y) is listed in a table (sums to 1). For continuous variables, a joint density f(x,y) is used (integral over a region gives that region’s probability).\n\n(2) Simple real-world example: Roll two fair dice. Let X be the first die and Y the second. Then P(X=i, Y=j) = 1/36 for i, j in {1,…,6}. The joint distribution shows all outcome pairs and their probabilities; X and Y are independent, so the joint probabilities factor into the margins.\n\n(3) Why it’s important: It shows how variables relate, not just individual chances. It lets us compute probabilities of combined events, understand dependence or correlation, and build models for prediction, risk, and decision-making in real-world problems."}
{"Major": "Statistics", "Term": "random variable", "Explanation": "- Basic meaning: A random variable is a rule that assigns a number to each possible outcome of a random process. It lets us turn randomness into a chartable, numerical value.\n\n- Simple real-world example: Roll a fair six-sided die. Let X be the number that shows (1–6). Each outcome has a known probability, and X is the numerical summary of the roll.\n\n- Why it’s important: It lets us describe uncertainty with numbers, compute probabilities (like P(X = 4)), and summarize results with averages and spread (expected value, variance). This is the foundation for building models, making predictions, and making informed decisions under uncertainty."}
{"Major": "Statistics", "Term": "confidence interval (CI)", "Explanation": "Confidence interval (CI) is a range of numbers that is likely to contain the true value you’re estimating, based on your data. It comes with a confidence level (often 95%), which reflects how often the method would capture the true value if you repeated the study many times.\n\nExample: A poll finds 52% support a policy, with a 95% CI of 49% to 55%. We’re “confident” that the true level of support lies between 49% and 55% (in repeated studies, about 95% of such intervals would include the true value).\n\nWhy it’s important:\n- It shows uncertainty, not a single exact number.\n- It helps avoid overclaiming precision.\n- It shows how sample size affects precision (larger samples give narrower intervals).\n- It aids comparison and decision making by framing what we don’t know as well as what we estimate."}
{"Major": "Statistics", "Term": "covariance", "Explanation": "- Basic idea: Covariance measures whether two variables tend to move together. If they rise and fall together, covariance is positive; if one tends to rise while the other falls, it’s negative; if there’s no consistent pattern, it’s near zero.\n\n- Simple real-world example: Hours studied and test scores. Generally, students who study more tend to get higher scores, so the covariance between study time and score is positive.\n\n- Why it matters: It’s a foundational way to quantify relationships between variables and is used in many analyses (e.g., regression, portfolio risk). Be mindful that covariance depends on the units of the variables, so its magnitude isn’t as easy to compare as correlation, which standardizes the measure. Also, covariance indicates association, not causation."}
{"Major": "Statistics", "Term": "likelihood function", "Explanation": "- Basic meaning: The likelihood function L(θ) shows how plausible different parameter values θ are, given the observed data and a statistical model. It is a function of θ (not a probability of θ itself) derived from P(data | θ).\n\n- Simple real-world example: Suppose you flip a coin 10 times and observe 7 heads. Let p be the chance of heads. The likelihood is L(p) ∝ p^7 (1−p)^3, viewed as a function of p. The value of p that maximizes L(p) is the most plausible coin bias (p-hat = 0.7).\n\n- Why it’s important: It’s the foundation of many methods to analyze data—estimating parameters (maximum likelihood estimation), comparing different models (likelihood ratios), and creating intervals of plausible values for the parameters. It helps translate observed data into informed conclusions about the underlying process."}
{"Major": "Statistics", "Term": "probability measure", "Explanation": "Probability measure\n\n- Basic meaning: A probability measure is a rule that assigns to each possible event (a set of outcomes) a number between 0 and 1, representing how likely that event is. It must say the impossible event has probability 0, the sure event has probability 1, and the probabilities add up for disjoint events.\n\n- Simple real-world example: Flip a fair coin. P(Heads) = 0.5, P(Tails) = 0.5, and P(Heads or Tails) = 1. If you draw one card from a standard deck, P(red card) = 26/52 = 0.5.\n\n- Why it’s important: It provides a consistent way to quantify uncertainty, so we can compute chances of any event by combining simple probabilities. This idea underpins statistics, data analysis, risk assessment, experiments, and informed decision-making."}
{"Major": "Statistics", "Term": "regression analysis", "Explanation": "- Basic meaning: Regression analysis is a set of statistical methods to study how a result (the outcome) changes when one or more factors (predictors) change. The common form, linear regression, fits a straight line to data to describe the relationship and to predict the outcome from the predictors.\n\n- Simple real-world example: Predicting house price from size. Gather data on many houses (price and square footage). The regression line shows how price tends to rise with more space, and by how much. You can use it to estimate a house price for a given size and to see the effect of other factors like location or age.\n\n- Why it is important: It helps forecast outcomes and quantify relationships, supporting data-driven decisions. It reveals which factors matter most, compares scenarios, and provides measures of uncertainty (confidence in predictions). Widely used in business, science, and policy."}
{"Major": "Statistics", "Term": "causal study", "Explanation": "(1) Basic meaning: A causal study asks whether changing X will cause a change in Y. It seeks a cause-and-effect link, not just a relationship or coincidence, and tries to rule out other factors that could explain the link.\n\n(2) Simple real-world example: A randomized controlled trial tests a new blood pressure drug. People are randomly assigned to receive the drug or a placebo. If the drug group shows lower blood pressure, and groups are otherwise similar, this supports a causal effect of the drug on lowering BP.\n\n(3) Why it is important: It helps determine what really works or causes harm, guiding medicine, policy, and personal decisions. Without establishing causality, we might misinterpret correlations as causes, leading to wasted resources or harmful choices. Common methods include randomized experiments and well-designed observational studies that control for confounding factors."}
