{"Major": "Statistics", "Term": "mutual independence", "Explanation": "Mutual independence (probability)\n\n- Basic meaning: A set of events A1, A2, ..., An are mutually independent if, no matter which subset you pick, the chance that all events in that subset happen is the product of their individual chances. In symbols: for any subset {i1,...,ik}, P(Ai1 ∩ ... ∩ Aik) = P(Ai1) × ... × P(Aik). Mutual independence implies pairwise independence, but the reverse is not always true.\n\n- Simple real-world example: Flip three fair coins. Let A = heads on flip 1, B = heads on flip 2, C = heads on flip 3. Each has P = 1/2. Since flips are independent, P(A ∩ B ∩ C) = 1/2 × 1/2 × 1/2 = 1/8, and P(A ∩ B) = 1/4, etc.\n\n- Why it matters: If events are mutually independent, you can multiply probabilities to find joint outcomes, making calculations and modeling much easier. Independence is a common assumption in statistics and experiments, but real data often aren’t truly independent, which can lead to incorrect conclusions."}
{"Major": "Statistics", "Term": "statistical inference", "Explanation": "1) Basic meaning\nStatistical inference is the process of using data from a sample to draw conclusions about a larger group (the population) and to quantify how uncertain those conclusions are.\n\n2) Simple real-world example\nIf you want the average height of adults in your city, you can’t measure everyone. Take a random sample of 200 adults, compute the average height from that sample, and use methods to estimate the city-wide average. You’ll also get a margin of error and a confidence level (e.g., 95%), which tell you how sure you are about the estimate.\n\n3) Why it is important\nMost questions are about populations, not every individual. Inference lets us learn from samples instead of counting everyone, guiding decisions in health, policy, and business. It also shows how reliable conclusions are and helps compare groups while accounting for random variation."}
{"Major": "Statistics", "Term": "joint distribution", "Explanation": "Joint distribution (of two variables X and Y) tells you how likely every possible combination of their values occurs together. For discrete variables, it’s a table P(X=x, Y=y) giving the probability of each pair. For continuous variables, it’s a joint density f(x, y) that you integrate over ranges to get probabilities.\n\nReal-world example: Hours studied (H) and exam score (S). The joint distribution assigns probabilities to pairs like (H=3, S=78), (H=0, S=50), etc. It shows not just how many hours people study on average, but how study and score relate—whether more studying tends to go with higher scores.\n\nWhy it’s important: It captures the relationship between variables, not just their separate behavior. From the joint distribution you can get margins (P(X=x), P(Y=y)), conditional probabilities (P(Y|X)), and test for independence (joint equals product of marginals). This underpins prediction, decision-making, and many statistical methods."}
{"Major": "Statistics", "Term": "random variable", "Explanation": "- Basic meaning: A random variable is a rule that assigns a number to every possible outcome of a random process. It turns outcomes into numbers we can analyze.\n\n- Simple real-world example: Roll a six-sided die. Let X be the number that lands face up. X can be 1, 2, 3, 4, 5, or 6 (each with probability 1/6).\n\n- Why it’s important: It lets us quantify uncertainty and do math with it—like finding the average (expected value), understanding variability (variance), and asking questions such as “what’s the chance X is at least 5?” Random variables are the foundation of probability models used in science, engineering, finance, and many everyday decisions."}
{"Major": "Statistics", "Term": "confidence interval (CI)", "Explanation": "Confidence interval (CI)\n\n- Basic meaning: A CI is a range around a sample estimate (like an average) that is likely to contain the true value for the whole population. It shows the uncertainty you have because you studied only a sample.\n\n- Simple real-world example: A survey finds the average daily caffeine intake is 200 mg, with a 95% CI of 180–220 mg. We are fairly confident the true average caffeine intake in the population lies somewhere between 180 and 220 mg. If we repeated the survey many times, about 95% of those calculated intervals would include the true average.\n\n- Why it is important: It communicates how precise the estimate is and how much we should trust it. It helps scientists, doctors, and policymakers make informed decisions and compare results. The interval’s width depends on sample size, variability, and chosen confidence level (e.g., 90%, 95%, 99%).\n\nNote: For a single fixed interval, the true value is either inside or not; the “95%” refers to the method, not to this one interval."}
{"Major": "Statistics", "Term": "covariance", "Explanation": "- Basic meaning: Covariance measures how two variables tend to move together. If it’s positive, they tend to rise (or fall) together; if negative, one tends to rise while the other falls. If near zero, there’s no clear pattern.\n\n- Simple real-world example: Temperature and ice cream sales. On hotter days, ice cream sales usually go up, so the two variables show positive covariance. Another example: hours studied and exam score (more studying often relates to higher scores).\n\n- Why it’s important: It helps reveal relationships between things and is used in science, economics, and data analysis. However, covariance depends on the units of measurement and is not easily interpretable by itself. For easier interpretation, we use correlation, a standardized version that ranges from -1 to 1."}
{"Major": "Statistics", "Term": "likelihood function", "Explanation": "Likelihood function\n\n- Basic meaning: The likelihood L(θ) is a way to measure how plausible different values of a parameter θ are, given the data you observed. It is a function of θ (the parameter), not of the data.\n\n- Simple real-world example: Suppose you’re unsure if a coin is biased. You flip it 10 times and get 7 heads. If θ is the probability of heads, the likelihood is L(θ) = θ^7 (1−θ)^3. The function reaches its maximum at θ = 0.7, so 70% heads is the most plausible value given the data (the maximum likelihood estimate).\n\n- Why it is important: It underpins how we estimate parameters (maximum likelihood estimation), compare different models, and build confidence intervals and tests. It lets us learn about unknown probabilities or effects from data, without making prior assumptions about θ. Note: it is not the probability that θ is true; it’s the probability of the observed data for each possible θ."}
{"Major": "Statistics", "Term": "probability measure", "Explanation": "- Basic meaning: A probability measure is a rule that assigns a number between 0 and 1 to every possible event (a set of outcomes) in a given situation. It says how likely each event is, with the whole sample space (all possible outcomes) having probability 1. It also follows additivity: if two events can’t happen at the same time, the probability of one or the other is the sum of their probabilities.\n\n- Simple real-world example: Think of rolling a fair six-sided die. Each single face has probability 1/6. The event “even number” has probability 3/6 = 1/2, and the event “not 3” has probability 5/6.\n\n- Why it is important: It provides a precise, consistent way to quantify uncertainty. With a probability measure, you can combine outcomes, compare likelihoods, and compute expected results, which underpins all statistics, data analysis, and decision making under risk."}
{"Major": "Statistics", "Term": "regression analysis", "Explanation": "Regression analysis\n\n(1) Basic meaning: A statistical method to study how a dependent variable (the outcome you care about) changes when one or more independent variables (factors you think matter) change. It fits a line or curve to data to describe that relationship and make predictions, while expressing uncertainty.\n\n(2) Simple real-world example: Suppose you want to predict exam scores from hours studied. Collect data from students, fit a line like score = intercept + slope × hours. If the slope is positive, more study tends to lead to higher scores. You can predict a score for a given study time and estimate the expected range of scores.\n\n(3) Why it is important: It helps forecast outcomes, quantify how strongly factors influence results, and inform decisions in education, business, health, and science. It also allows controlling for multiple factors. Important caveats: correlation does not prove causation, and models depend on data quality and assumptions."}
{"Major": "Statistics", "Term": "causal study", "Explanation": "Causal study (in statistics)\n1) Basic meaning: A causal study tries to show that changing one thing (the cause) makes another thing change (the effect), not just that they happen together. It aims to identify true cause-and-effect, often using experiments with random assignment or strong statistical methods to control for other factors.\n\n2) Simple real-world example: A medical trial tests a new drug vs. a placebo. Patients are randomly assigned to receive either the drug or the placebo. After a fixed period, researchers compare outcomes (e.g., blood pressure). If the drug group improves more, and randomization balanced other factors, we infer the drug causes the improvement.\n\n3) Why it’s important: Knowing causal relationships helps people and organizations make effective decisions—whether to approve a medicine, change a policy, or improve a product. Without causal proof, we might misinterpret correlations as causes. Randomization and proper controls help ensure observed effects are truly due to the proposed cause."}
