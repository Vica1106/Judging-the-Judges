{"Major": "Computer Science", "Term": "quantum computing", "Explanation": "- Basic meaning: Quantum computing uses quantum bits, or qubits, that can be 0, 1, or both at once (superposition). Qubits can be entangled, so their states are interconnected. This lets certain problems be processed in fundamentally different, parallel ways than with regular computers.\n\n- Simple real-world example: Imagine optimizing delivery routes for many trucks. A quantum computer could explore many route options at once and help find a very good (or optimal) plan faster than a classical computer. It could also speed up scientists’ work designing new drugs or materials by checking many possibilities simultaneously.\n\n- Why it is important: It could dramatically speed up tasks like factoring large numbers (affecting encryption), simulating complex molecules, and solving hard optimization problems. This could unlock advances in medicine, energy, logistics, and security. Right now, quantum computers are still early-stage and will complement, not yet replace, classical computers."}
{"Major": "Computer Science", "Term": "big O notation", "Explanation": "Big O notation is a way to describe how the time or memory an algorithm uses grows as the amount of data grows. It focuses on the growth rate (how it scales) and ignores tiny details like constant factors.\n\nSimple real-world example:\n- Searching a list of n items by checking each one until you find a match takes about n checks in the worst case. This is O(n) time.\n- If the list is sorted, you can use binary search and cut the search size in half each step, about log2(n) checks. This is O(log n) time.\n\nWhy it’s important:\n- it lets you compare different approaches and predict how performance will scale as data grows\n- helps you choose more efficient algorithms and estimate costs for large datasets\n- guides memory usage planning and scalability decisions\n\nIn short: Big O is a simple way to talk about how an algorithm’s resource needs grow with bigger inputs."}
{"Major": "Computer Science", "Term": "semantics", "Explanation": "Semantics in computer science means the meaning or behavior of code and data: what a program does when it runs, not just how it is written (the syntax).\n\nA simple real-world example: consider the statement x = x + 1. Its semantics are that, when executed, it increases x by 1. Different languages might use different words or symbols for the same idea, but the intended effect should be the same. Conversely, x = 0 has different semantics than x = x + 1.\n\nWhy it’s important: semantics lets us reason about and verify what a program will do, beyond its appearance. It helps with debugging, writing correct code, and provides the basis for compiler optimizations and translating programs between languages. It also matters for accessibility and interoperability, where the intended meaning must be preserved across systems."}
{"Major": "Computer Science", "Term": "floating-point arithmetic", "Explanation": "Floating-point arithmetic is how computers store and compute real numbers with a “floating” decimal point.\n\n- Basic meaning: A number is stored as a sign, a mantissa (the digits), and an exponent (how far to move the decimal). This is like scientific notation, allowing very large or tiny numbers with a fixed amount of memory.\n\n- Simple real-world example: 123.45 can be stored roughly as 1.2345 × 10^2; 0.0000123 as 1.23 × 10^-5. In practice, binary floats have about 7 digits (32-bit) or 15 digits (64-bit) of precision, and some decimals can’t be represented exactly in binary.\n\n- Why it’s important: It enables fast calculations across wide ranges of numbers in science, graphics, and machine learning. But it’s approximate: rounding errors occur, and exact values (like 0.1) may not be stored precisely. For money or exact comparisons, use special decimal types or apply tolerance."}
{"Major": "Computer Science", "Term": "quicksort", "Explanation": "- Basic meaning: Quicksort is a fast sorting algorithm. It picks a pivot item, partitions the remaining items into those smaller than the pivot and those larger, then recursively sorts the two groups until the whole list is ordered.\n\n- Simple real-world example: Sorting a deck of cards. Pick a pivot card (say 7). Move all cards with value less than 7 to the left and all greater cards to the right. Then repeat on each side until every card is in order.\n\n- Why it’s important: Quicksort is often very fast on large lists and typically runs in about n log n time on average. It sorts in place, using little extra memory, and it demonstrates the powerful divide-and-conquer idea behind many efficient algorithms. It’s widely taught and used in software libraries."}
{"Major": "Computer Science", "Term": "agent-based model (ABM)", "Explanation": "ABM (Agent-Based Model) is a computer simulation approach to study complex systems. It models many autonomous “agents” (people, vehicles, animals, etc.), each with simple rules for behavior and interaction with others and the environment. System-wide patterns emerge from these local interactions.\n\nSimple real-world example: Modeling shoppers in a store. Each shopper (agent) has goals (find items), a budget, and reacts to nearby shoppers and shelves. Their individual choices can lead to queues, crowding, and the effect of promotions on overall buying.\n\nWhy it matters: ABMs let us study how complex, adaptive systems behave when lots of individuals interact, especially when no single equation captures the whole picture. They’re useful for testing policies and designs in traffic, crowd safety, disease spread, economics, ecology, and more—often revealing counterintuitive outcomes before real-world trials."}
{"Major": "Computer Science", "Term": "big data", "Explanation": "Big data\n\n- Basic meaning: Very large, fast-moving, and diverse data sets that aren’t easy to store or analyze with ordinary tools. It focuses on volume, velocity, and variety (and sometimes veracity/value) to pull out useful insights.\n\n- Simple real-world example: A streaming video service tracks millions of users’ viewing histories, searches, likes, and playback times daily. Analyzing this helps personalized recommendations, decides which content to promote, and optimizes servers to reduce buffering.\n\n- Why it’s important: It lets organizations make better decisions, tailor products and services, and spot trends or problems (like fraud or disease outbreaks) at scale. It also enables innovation across sectors, though it requires careful attention to data quality, privacy, and the right tools and skills."}
{"Major": "Computer Science", "Term": "class", "Explanation": "1) Basic meaning: In object-oriented programming, a class is a blueprint for creating objects. It defines what data the object stores (attributes) and what actions it can perform (methods). An actual object created from the class is called an instance.\n\n2) Simple real-world example: A Car class. Attributes might include color, make, model, and currentSpeed. Methods might include start(), honk(), accelerate(), and brake(). From this blueprint, you can create individual cars (instances) like a red Toyota Corolla with its own speed, all following the same structure and actions.\n\n3) Why it matters: Classes organize code around real-world ideas, making programs easier to read, reuse, and maintain. They enable building complex systems from simple parts, support code reuse through inheritance, and help manage large projects by promoting consistency and abstraction."}
{"Major": "Computer Science", "Term": "coding theory", "Explanation": "1) Basic meaning\nCoding theory studies how to turn information into a code and add extra bits so errors from noise can be detected and corrected. The aim is to design codes that are reliable but not wasteful with too much extra data.\n\n2) Simple real-world example\nQR codes use error-correcting codes. Even if part of the code is dirty or damaged, the extra bits let scanners reconstruct the original data.\n\n3) Why it is important\nIt makes communication and storage reliable in imperfect conditions—phone signals, Wi‑Fi, CDs/DVDs, and cloud storage. It helps data stay accurate and safe, underpinning everyday tech and critical systems alike."}
{"Major": "Computer Science", "Term": "computability theory", "Explanation": "Computability theory\n\n- (1) Basic meaning: It studies which problems can be solved by a computer using a finite step-by-step procedure (an algorithm). It uses abstract models (like Turing machines) to ask: is there an algorithm that will always give the right answer for every input?\n\n- (2) Simple real-world example: The halting problem—can there be a universal program that, for any other program and its input, says whether that program will ever stop or run forever? In general, no. This shows that some questions about software can’t be settled automatically in all cases.\n\n- (3) Why it’s important: It reveals the fundamental limits of what computers can do. It helps us understand which tasks can be automated, guides the design of reliable software and verification tools, and informs areas like cryptography and complexity by showing what is theoretically possible versus impossible."}
