{"Major": "Artificial Intelligence", "Term": "Selective Linear Definite Clause Resolution", "Explanation": "1. One-sentence definition\nSelective Linear Definite Clause Resolution (SLD resolution) is a goal-directed inference method in logic programming that repeatedly selects one atom from the current goal and resolves it with a definite clause to derive simpler subgoals, until the goal is proven or no rule applies.\n\n2. Simple intuition based on the definition\nThink of solving a problem by following one lead at a time: use a rule to turn that lead into easier questions, and repeat until you reach a final yes/no answer.\n\n3. Key components of the topic and related concepts\n- Definite clauses (Horn clauses with a single head).  \n- Goal (the query you want to prove).  \n- Selection function (chooses which goal literal to resolve).  \n- Unification (make the clause head match the chosen literal).  \n- Subgoals (the body becomes new goals).  \n- Backtracking/SLD-tree (the search structure; used in Prolog).  \n- Note: termination and completeness depend on the program structure.\n\n4. Clear real-world analogy\nAnalogy: a librarian helping you find a book. Your question is the goal. You pick one relevant rule (clause) whose head matches your question (unification), generating new questions (subgoals). If a path fails, you backtrack and try another rule; the overall exploration forms an SLD-tree. This mirrors selecting and resolving literals step by step.\n\n5. Common misconception or confusion\n misperception: it reasons with all rules at once or guarantees results for every program. Reality: it’s selective, may not terminate, and is complete only for certain finite, definite Horn programs."}
{"Major": "Artificial Intelligence", "Term": "Big O notation", "Explanation": "1) One-sentence definition\nBig O notation expresses how the worst-case running time or memory usage of an algorithm grows as input size n increases, focusing on the dominant term and ignoring constants.\n\n2) Simple intuition based on the definition\nAs inputs get larger, the growth pattern matters more than exact timings. Constant factors and small terms fade away, so two algorithms with the same growth rate feel similar for large n.\n\n3) Key components of the topic and related concepts\nn = input size; T(n) = time/space. Common classes: O(1), O(log n), O(n), O(n log n), O(n^2). Worst-case focus. Related concepts: Theta (tight bound), Omega (lower bound).\n\n4) Clear real-world analogy\nAnalogy: finding a book on a shelf. If you know the exact position, it's O(1). If you scan left to right, it's O(n). If the shelf is sorted and you binary-search, it's O(log n). Map: n = number of books; steps = checks; growth reflects time with increasing shelf size.\n\n5) Common misconception or confusion\nBig O is not the exact run time. It’s an upper bound that ignores constants and hardware; it may differ from average-case and from Theta/Omega. Use Big O to compare growth rates, not precise timings."}
{"Major": "Artificial Intelligence", "Term": "neural machine translation (NMT)", "Explanation": "1. One-sentence definition\nNeural machine translation (NMT) is a neural-network-based method that translates text by encoding a whole source sentence and then decoding it into a fluent target sentence.\n\n2. Simple intuition based on the definition\nIt's like a translator who reads a sentence in one language and then writes a natural, context-aware version in another, rather than translating word by word.\n\n3. Key components of the topic and related concepts\n- Encoder: reads and encodes the source sentence\n- Decoder: generates the target sentence\n- Attention: focuses on relevant parts of the source during translation\n- End-to-end training on bilingual data\n- Loss function (e.g., cross-entropy) and subword vocab (BPE)\n- Sequence-to-sequence learning\n\n4. Clear real-world analogy\nAnalogy: a chef translating a recipe.\n- Encoder = chef tasting ingredients (reads and encodes the source)\n- Attention = focusing on key spices (aligns source words to target)\n- Decoder = chef writing the new recipe steps (generates the target)\n- Training data = cookbook of bilingual examples\n- Overall quality = how well the chef preserves meaning and flow\n\n5. Common misconception or confusion\nNMT does not “understand” language like humans and cannot guarantee perfect translations. It learns patterns from data and can falter with rare words, idioms, or out-of-domain topics."}
{"Major": "Artificial Intelligence", "Term": "NP-hardness", "Explanation": "1. One-sentence definition: NP-hardness means every problem in NP can be reduced to the given problem in polynomial time; the problem is at least as hard as the hardest NP problems (and may or may not be in NP).\n\n2. Simple intuition: If you could solve the NP-hard problem quickly, you could solve any NP problem quickly by translating it into that problem first (the translation costs only polynomial time).\n\n3. Key components of the topic and related concepts:\n- Polynomial-time reductions from any NP problem to the target.\n- NP vs NP-hard vs NP-complete: NP-hard means at least as hard as NP; if the target is also in NP, it’s NP-complete.\n- Examples: SAT is NP-complete; many optimization problems (e.g., TSP optimization, Knapsack) are NP-hard; decision versions are often NP-complete.\n- Implication: proving NP-hardness suggests no efficient algorithm is known unless P=NP.\n\n4. Clear real-world analogy:\n- Analogy: a universal puzzle box. Transform any problem’s instance into this box’s configuration (reduction). If you had a fast solver for the box, you’d solve all NP problems by first translating them and then solving the box.\n- Mapping: translation = polynomial-time reduction; box = NP-hard problem; fast solver = implies P=NP if the box is in NP.\n\n5. Common misconception or confusion:\n- NP-hard does not mean “unsolvable” or beyond help; many NP-hard problems have practical heuristics and exact methods. Also, NP-hard ≠ NP-complete unless the problem is also in NP."}
{"Major": "Artificial Intelligence", "Term": "true quantified Boolean formula", "Explanation": "1. **One-sentence definition**: A true quantified Boolean formula (TQBF) is a closed Boolean formula with a prefix of quantifiers over Boolean variables that evaluates to true under the standard ∀/∃ semantics.\n\n2. **Simple intuition based on the definition**: Think of a game where the universal player sets some bits, the existential player responds with the rest, and the final condition φ(x,y) must hold for the existential side to win. If the universal side can force φ to be false, the formula is false.\n\n3. **Key components of the topic and related concepts**: Variables with quantifiers (∃, ∀) forming a prefix, a quantifier-free matrix φ (the Boolean body), the truth-conditions given by the quantifier order, and the fact that TQBF denotes the set of true closed QBF formulas (a PSPACE-complete decision problem).\n\n4. **Clear real-world analogy**: A lock-and-key puzzle: the guard (∀) chooses some knobs (x); the solver (∃) chooses the remaining knobs (y); the mechanism works if there exists a way to set y for every x so that φ(x,y) holds. Mapping: universal moves = guard’s choices, existential moves = solver’s choices, matrix = the mechanism’s constraint.\n\n5. **Common misconception or confusion**: It’s not a propositional tautology; truth depends on the quantifier order. It’s not just SAT; deciding TQBF asks whether a given quantified sentence is true, a PSPACE-complete problem."}
{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "1) One-sentence definition\nAlgorithmic probability (Solomonoff probability) assigns to a string x the total probability that a random program on a fixed universal Turing machine outputs x; each program p contributes 2^{-|p|} to the sum.\n\n2) Simple intuition based on the definition\nIf you generate programs by flipping random bits, shorter (simpler) programs are far more common, so outputs produced by those short programs are collectively more probable.\n\n3) Key components of the topic and related concepts\n- Universal Turing machine (fixed reference)\n- Random programs with prior 2^{-|p|} (shorter programs heavier)\n- P(x) = sum_{p: U(p)=x} 2^{-|p|}\n- Relation to Kolmogorov complexity K(x) ≈ -log P(x)\n- Part of Solomonoff induction; the prior is incomputable in general\n\n4) Clear real-world analogy\nThink of a library of script \"recipes\" that generate patterns. If you pick a script at random by flipping bits, many short scripts can produce the same pattern. The pattern’s probability is the sum of weights 2^{-|p|} over all scripts that yield it. Short scripts correspond to small K(x), and many scripts producing the same output reflect multiple descriptions.\n\n5) Common misconception or confusion\nIt is not the observed frequency of data in nature, nor a practical predictor. It’s a theoretical, uncomputable prior over strings based on program length, not a concrete probability you can compute or apply directly."}
{"Major": "Artificial Intelligence", "Term": "behavior informatics (BI)", "Explanation": "1. One-sentence definition\nBehavior informatics (BI) is the interdisciplinary study of collecting, modeling, and analyzing data about human behavior to understand, predict, and influence actions using computational methods (often AI/ML).\n\n2. Simple intuition based on the definition\nThink of BI as a smart assistant that observes your choices, learns your patterns, and suggests useful next steps—like an app that tailors recommendations to your habits.\n\n3. Key components of BI (and related concepts)\n- Data sources: sensors, app logs, surveys, etc.\n- Behavior models: representations of habits, contexts, and sequences\n- Analytics/AI: pattern discovery, prediction, and causal inference\n- Interventions/UX: adaptive interfaces, nudges, personalized guidance\n- Ethics/privacy/governance: fairness, consent, data protection\nRelated concepts: human–computer interaction, user modeling, behavioral analytics.\n\n4. Clear real-world analogy\nAnalogy: BI is like a personal fitness coach.\n- Data collection → trackers log activity\n- Behavior models → coach builds a habit map\n- Prediction → forecasts next-day activity\n- Interventions → reminders and goal adjustments\n- Feedback loop → measures results and updates the plan\nMapping: BI turns raw data into models, uses them to predict actions, and designs targeted interventions, while continuously refining with feedback.\n\n5. Common misconception or confusion\nMistakenly: BI can read minds or guarantee perfect outcomes. Reality: BI provides probabilistic insights under uncertainty and requires careful data, context, and ethical safeguards."}
{"Major": "Artificial Intelligence", "Term": "big data", "Explanation": "1. One-sentence definition: Big data are data sets so large, fast, or varied that traditional tools can't process them efficiently, requiring new architectures and analytics.\n\n2. Simple intuition based on the definition: Imagine a constant stream of online posts, transactions, and sensor readings—endless data you can't manage with a single spreadsheet.\n\n3. Key components of the topic and related concepts: Core elements include Volume (how much data), Velocity (speed of data generation), Variety (data types), Veracity (data quality), plus tools (Hadoop, Spark), data types (structured, semi-structured, unstructured), analytics (descriptive to prescriptive), and governance/privacy.\n\n4. Clear real-world analogy: Analogy: a city’s traffic system. Data are cars; sources are streets; storage is the roads; processing is the traffic signals; insights are optimized routes. Mapping: volume = number of cars; velocity = speed; variety = vehicle types; veracity = data accuracy; tools = cameras/storage/compute platforms; analytics = route optimization; governance = traffic rules.\n\n5. Common misconception or confusion: “More data automatically means better insights.” Reality: data quality, relevant questions, and proper methods matter; privacy, governance, and appropriate analytics are essential."}
{"Major": "Artificial Intelligence", "Term": "convolutional neural network", "Explanation": "1. One-sentence definition\nA convolutional neural network (CNN) is a neural network designed for grid-structured data (like images) that uses learnable filters sliding over the input to detect local patterns and build hierarchical features.\n\n2. Simple intuition based on the definition\nA small window scans across an image to spot simple cues (edges, textures); stacking many scans helps the model recognize complex objects by combining these cues.\n\n3. Key components of the topic and related concepts\nConvolutional layers with kernels, activation functions (e.g., ReLU), pooling layers (max/average), strides and padding, and multiple stacked layers producing feature maps, followed by a classifier head. Related ideas: receptive field, parameter sharing, data augmentation, and transfer learning.\n\n4. Clear real-world analogy\nAnalogy: a set of pattern-spotting templates moves over a photo. Early templates detect simple marks (edges); later templates combine marks into parts and whole objects. In tech terms: filters are templates, feature maps are highlighted regions, pooling creates summaries, and deeper layers capture complex concepts.\n\n5. Common misconception or confusion\nMistaken idea: CNNs “understand” images like humans. They learn statistical patterns from grid-structured data, require lots of labeled data, and rely on architecture choices (filters, pooling) rather than general intuition. They don’t automatically handle arbitrary input sizes without adjustment."}
{"Major": "Artificial Intelligence", "Term": "Darkforest", "Explanation": "1. One-sentence definition: Dark Forest is a speculative idea from science fiction that cosmic civilizations stay quiet and may strike others first to ensure their own survival, turning the universe into a dangerous Dark Forest.\n\n2. Simple intuition based on the definition: If you’re in a dark forest with hidden predators, revealing yourself is risky. To survive, you stay quiet, gather information, and deter or preempt potential threats.\n\n3. Key components of the topic and related concepts:\n- Detection risk: any detectable signal can invite attack.\n- Concealment/deterrence: minimize emissions, mislead, or project power without revealing intent.\n- Preemption: a rational option to strike first if threat seems imminent.\n- Consequences: pervasive caution, misinterpretation, and potential escalation.\n- Related ideas: Fermi paradox, signaling, game theory (deterrence, escalation).\n\n4. Clear real-world analogy: Imagining nations in a dangerous neighborhood. Revealing capabilities invites targets; they hide or deter and may threaten or strike preemptively. \n- Dark Forest maps to deterrence and risk assessment.\n- Detection signals correspond to intelligence or overt displays.\n- Concealment to secrecy or stealth.\n- Preemption to preventive war logic.\n- Silence to mistrust and escalations.\n\n5. Common misconception or confusion: It’s a fictional metaphor, not a proven fact about real civilizations or aliens. It highlights strategic thinking about detection and survival, not a universal law."}