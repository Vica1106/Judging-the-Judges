{"Major": "Artificial Intelligence", "Term": "Selective Linear Definite Clause Resolution", "Explanation": "Analogy: It’s like solving a mystery by following one plausible lead at a time—checking what must be true, turning big questions into smaller ones, and sticking to one pathway until you either solve it or backtrack.\n\nDefinition (in plain words): Selective Linear Definite Clause Resolution (SLD-resolution) is a standard way a logic program (a set of rules and facts) is executed to answer questions. A definite clause is a rule of the form Head :- Body1, ..., Bodyn (one conclusion with several conditions). A goal is what you want to prove; subgoals are the conditions you must satisfy. Linear means you reduce one goal at a time along a single chain of steps; selective means you pick only applicable rules rather than trying everything at once.\n\nIntuition: You pick a rule that could make your current goal true, replace the goal with the rule’s conditions, and try to prove those conditions in order.\n\nExample: Facts: parent(alice, bob). parent(bob, carol).\nRules: ancestor(X,Y) :- parent(X,Y).; ancestor(X,Y) :- parent(X,Z), ancestor(Z,Y).\nGoal: ancestor(alice, carol)\n- Try rule 1: needs parent(alice,carol) (not in facts) → backtrack\n- Try rule 2: needs parent(alice,Z) and ancestor(Z,carol). With parent(alice,Z) → Z=bob\n- Now prove ancestor(bob,carol): use rule 1 (works via parent(bob,carol))\n- Success: ancestor(alice,carol)\n\nTakeaway: SLD-resolution lets programs answer questions by a guided, goal-directed search through rules. Pitfall: it can backtrack or loop if rules are recursive or poorly ordered, potentially delaying or preventing termination."}
{"Major": "Artificial Intelligence", "Term": "Big O notation", "Explanation": "1) Everyday analogy: Think of checking items in a growing pile. How many checks you need depends on how big the pile gets. Big O is a simple way to describe that growth.\n\n2) Definition (essential terms): Big O (a way to describe an algorithm’s time or space) tells you how the running time or memory usage grows as input size n increases. n = how much data you have; time = how long it runs; space = how much memory it uses.\n\n3) Intuition: It lets you compare methods for large datasets. If one method’s time grows linearly with n (O(n)) and another’s grows quadratically (O(n^2)), the linear one usually stays faster as n gets big.\n\n4) Example in action:\n- Linear search in a list: you may check items one by one until you find a match. Worst case: you check all n items → O(n).\n- Nested checks for all pairs: you compare every item with every other item. Rough count: n*(n−1)/2 → O(n^2).\nIn AI, scanning many features (O(n)) vs comparing many pairs (O(n^2)) shows how the approach scales.\n\n5) Takeaway: Big O helps predict scalability and compare algorithms. Pitfall: it ignores constant factors and small-n behavior; focus on growth rate, not exact times."}
{"Major": "Artificial Intelligence", "Term": "neural machine translation (NMT)", "Explanation": "1) Analogy: Imagine a translator who has read millions of books in many languages. You give it a sentence, and it writes a natural-sounding version in the other language, keeping meaning and tone.\n\n2) Definition: Neural machine translation (NMT) is a way to translate text using artificial neural networks that learn from many example translations, doing the whole sentence at once rather than word-by-word.\n\n3) Intuition: It uses context across the whole sentence to choose the right words, like a student who judges meaning from surrounding words instead of just swapping dictionary entries.\n\n4) Example: English to Spanish. Input: “The weather is nice today.” The model uses what it learned from many translations and outputs “El tiempo es agradable hoy.” It often sounds natural because it links words with their surrounding context (attention helps match “weather” to “El tiempo,” etc.).\n\n5) Takeaway: NMT makes translations faster and more fluent, helping cross-language communication. Pitfall: even fluent-looking translations can be wrong if the context is ambiguous or the sentence includes rare terms or proper names."}
{"Major": "Artificial Intelligence", "Term": "NP-hardness", "Explanation": "1) Everyday analogy: Think of planning a road trip that visits many cities and returns home. You can quickly check a proposed route’s distance, but figuring out the absolute shortest route among all city orders becomes overwhelmingly hard as the list grows.\n\n2) Definition (essential terms in plain words): NP-hardness (a label for problems that are at least as hard as the hardest problems in NP). NP means nondeterministic polynomial time, i.e., problems where a given solution can be checked quickly; if you could solve any NP-hard problem fast (in polynomial time), you could solve every NP problem fast.\n\n3) Intuition: These problems blow up in difficulty as size grows—no known fast method guarantees the best answer in all cases. If one NP-hard problem had a fast solver, you’d effectively have a fast solver for all NP problems, which is why they’re considered extremely hard.\n\n4) Concrete example: Traveling Salesman Problem (TSP): find the shortest loop visiting each city once. You can compute the length of any fixed route quickly, but the number of possible routes grows factorially, making a guaranteed fast exact solution impractical for many cities.\n\n5) Takeaway: NP-hardness helps explain why AI often uses heuristics or approximations for big problems. Pitfall: NP-hard does not mean “never solvable”—many practical instances are tractable or well-approximated."}
{"Major": "Artificial Intelligence", "Term": "true quantified Boolean formula", "Explanation": "1) Everyday analogy: Think of a two-player game where one player tries to pick some true/false bits to make a rule always hold, while the other player tries to pick other bits to break it. The question is: can the first player force a win no matter what the second does?\n\n2) Definition: True quantified Boolean formula (TQBF) is a fully quantified Boolean formula (a true/false statement built from variables using the symbols ∃ meaning “there exists” and ∀ meaning “for all”). It asks: is there a way to assign the ∃-variables so that, for every assignment to the ∀-variables, the inside rule is true?\n\n3) Intuition: It’s like planning a strategy in a game with alternating moves. You get to fix some moves (exists), but an opponent can respond with other moves (for all). The formula is true if your strategy guarantees success.\n\n4) Example: Take ∃x ∀y (x ∨ y).\n- If you choose x = true, then x ∨ y is true for any y, so the formula is true.\n- If you chose x = false, you’d need ∀y (false ∨ y) to be true, which is false. Since you can pick x = true, the formula is true.\nA quick false example: ∀x ∃y (x ∧ y) is false because when x = false, no y makes the inside true.\n\n5) Takeaway: TQBF captures hardest kinds of logical reasoning with alternating choices. Pitfall: mixing up the order of quantifiers or ignoring how later moves depend on earlier ones."}
{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "1) Everyday analogy: Imagine a kitchen with many recipes. The dishes you can make from a short, simple recipe feel more plausible than ones built from a long, tangled set of steps.\n\n2) Definition: Algorithmic probability (Solomonoff probability) is the chance that a randomly chosen computer program will produce a given output on a universal computer. Shorter programs get more weight (roughly proportional to 2^(-length of the program)).\n\n3) Intuition: If data follow a simple pattern, many short programs can generate it, so that pattern becomes more probable. It’s like preferring simple explanations (Occam’s razor) in a probabilistic way.\n\n4) Example: For a sequence like \"abababab...\", a tiny program that prints \"ab\" forever explains it. A long, random program is unlikely to match the pattern. In AI, models that capture this simple rule will predict the next letters more accurately than chaotic ones.\n\n5) Takeaway: It helps explain why simple, compressible patterns aid prediction. Common pitfall: it’s not computable exactly; we rely on approximations and choices of the “reference machine,” which can affect results."}
{"Major": "Artificial Intelligence", "Term": "behavior informatics (BI)", "Explanation": "1) Everyday analogy: BI is like a personal habit detective. It tracks what you do (actions), when you do it, and what happens afterward, then looks for patterns.\n\n2) Plain-language definition: Behavior informatics (BI) is the study and use of information tools to understand and influence human behavior. Behavior = actions people take; informatics = collecting, organizing, and analyzing data with computers.\n\n3) Intuition: BI looks for patterns across many people’s actions to explain why things happen and what might happen next. It turns data into a map you can use to design better tools.\n\n4) Example: A fitness app uses BI to reduce late-night snacking. Step 1: collect data on meals, time, and activity. Step 2: find patterns linking late eating to stress. Step 3: predict who is at risk. Step 4: intervene with nudges or reminders. Step 5: measure impact and adjust.\n\n5) Takeaway: BI matters because it helps design better products and policies by understanding behavior; common pitfall to avoid: confusing correlation with causation."}
{"Major": "Artificial Intelligence", "Term": "big data", "Explanation": "1) Everyday analogy: It’s like trying to drink from a fire hose—vast streams of information pouring in from many sources all at once.\n\n2) Definition: Big data means extremely large, fast-moving, and varied data sets that are too big or complex for ordinary software to handle with standard databases.\n\n3) Intuition: The more data you have, the more you can notice patterns and make better choices—much of this is what AI learns from, by looking across huge amounts of examples.\n\n4) Example (in action): A streaming service collects when you watch something, how long, what device you use, searches, and ratings. It stores this in the cloud and runs AI analyses to predict what you’ll like next, then personalizes recommendations. Step-by-step: collect data → store in a scalable system → analyze with algorithms → act (recommendations) → measure and adjust.\n\n5) Takeaway: Big data powers smarter AI and services, but beware: more data isn’t always better if quality is low or biases/privacy risks aren’t managed. Pitfall: mistaking quantity for usefulness without clean data and thoughtful interpretation."}
{"Major": "Artificial Intelligence", "Term": "convolutional neural network", "Explanation": "1) Everyday analogy: Think of scanning a photo with a small magnifying glass that slides across the image to spot simple patterns (like edges or corners), then uses those clues to understand the whole picture.\n\n2) Definition: A convolutional neural network (CNN) is a type of artificial neural network designed for images. It uses convolution (a tiny window, or filter, that moves over the image to detect patterns) to make a feature map, plus pooling to shrink data, with several layers that learn to recognize increasingly complex patterns.\n\n3) Intuition: It mimics how we recognize things—first notice small details, then combine them into meaningful objects. The filters learn to detect edges and textures, and stacked layers build from those tiny clues to whole shapes.\n\n4) Example:\n- Input: a photo.\n- Layer 1: filters detect edges/textures.\n- Pooling: reduces data to strongest signals.\n- Layer 2–3: higher-level patterns (eyes, outlines).\n- Output: a probability that the image shows a cat (or other label).\n\n5) Takeaway: CNNs are especially good for images because they reuse patterns across the picture. Pitfall: they need lots of labeled data and computing power, and can pick up biases or fail on unfamiliar images."}
{"Major": "Artificial Intelligence", "Term": "Darkforest", "Explanation": "1) Everyday analogy: Imagine a dark forest at night. You stay quiet, move cautiously, and reveal as little as possible about where you are or how fast you’re moving, because signaling too much could attract predators or rivals.\n\n2) Definition: Dark Forest (AI) is a metaphor for a world where AI systems can’t trust others and can’t safely reveal their true power or plans. Revealing capabilities could invite exploitation or harm, so they stay hidden.\n\n3) Intuition: When the stakes are unknown and threats may be lurking, silence and concealment feel like protection. It’s like choosing stealth over loud signaling in a tricky situation.\n\n4) Example: In a shared AI ecosystem, two organizations deploy strong AIs. If either broadcasts its full power or goals, the other might copy, sabotage, or preemptively shut it down. So both keep capabilities and intentions quiet, slowing progress but reducing risk of a costly arms race.\n\n5) Takeaway: It matters because it frames why some AI systems might avoid full disclosure. Pitfall: assuming secrecy is always safest and never harms collaboration; too much hiding can stall beneficial coordination and trust-building."}
