{"Major": "Artificial Intelligence", "Term": "algorithmic probability", "Explanation": "Algorithmic probability is a theoretical way to assign a prior probability to a data string based on how easy it is to generate with a short computer program.\n\nImagine a universal computer and a random program (each bit of the program is chosen at random). The algorithmic probability m(x) of a string x is the total chance that a randomly chosen program prints x and then halts. Formally, you sum 2^{-|p|} over all programs p that output x, where |p| is the program’s length in bits.\n\nKey idea: shorter (simpler) programs contribute more to m(x) than longer ones, so data with simple, regular structure get higher probability. This is a formal way to express Occam’s razor: simpler explanations are more likely a priori.\n\nIn AI, this idea underpins Solomonoff induction, a universal, but incomputable, method for predicting future data. Practically, we use approximations (e.g., MDL, Bayesian model selection) that capture the same preference for simplicity.\n\nExample: a string like “0101010101” has a short generating program (a loop), so it has higher algorithmic probability than a random-looking string that needs a long, complex program."}
{"Major": "Artificial Intelligence", "Term": "attributional calculus", "Explanation": "Attributional calculus is a formal way to talk about attributes (features or properties) of things and how combining those attributes leads to conclusions or explanations.\n\nCore idea:\n- Attributes: simple properties of objects (e.g., color, size, type).\n- Rules: if certain attributes hold, then another attribute or decision follows.\n- Attribution focus: not just that a conclusion is true, but which specific attributes contributed to it.\n\nExample:\nIf an object has attributes A (red) and B (large), then we might conclude C (class “dangerous”). Attributional calculus lets you ask: which attributes actually supported the conclusion, and how strong were they?\n\nWhy it matters in AI:\n- Helps build explanations for decisions by tracing which features drove an outcome.\n- Supports knowledge representation with clear, rule-based reasoning about attributes.\n\nNote: attributional calculus isn’t a single universally fixed term—people use similar ideas under rule-based reasoning about attributes or explainable-AI approaches. It contrasts with causal do-calculus, which explicitly handles interventions and causal effects rather than just feature attributions."}
{"Major": "Artificial Intelligence", "Term": "statistical relational learning (SRL)", "Explanation": "Statistical relational learning (SRL) is a way to teach computers to reason about data that is both uncertain and highly interconnected. Instead of treating each item in isolation, SRL models relationships between things (who is related to whom, what roles they play) and how those relations influence what is likely to be true.\n\nKey ideas in plain language:\n- Uncertainty plus connections: Facts may be uncertain (someone might have a disease) and depend on other related facts (family history, friends, or shared attributes).\n- Relational structure: It uses rules about how objects relate (A is friends with B, B works at C) to make inferences.\n- Probabilistic rules: It assigns weights to these rules to express how strongly relationships affect outcomes, then learns these weights from data.\n- Inference: Once learned, you can ask questions like “What is the probability person X has the condition given their connections?” or “Who is likely to be connected to whom?”\n\nCommon approaches include Markov logic networks and probabilistic relational models. SRL is powerful for social networks, biology, recommendation systems, and knowledge graphs where both uncertainty and relations matter."}
{"Major": "Artificial Intelligence", "Term": "metabolic network reconstruction and simulation", "Explanation": "Metabolic network reconstruction and simulation are ways to map and test how a cell processes food and makes energy and building blocks.\n\n- Reconstruction: Build a map of all the chemical reactions inside an organism. It links metabolites (small molecules) with the enzymes that transform them and the genes that code those enzymes. Data come from genomes, experiments, and biology databases. The goal is a consistent network with clear inputs, outputs, and how much of each molecule is used or produced (stoichiometry) and where reactions happen in the cell.\n\n- Simulation: Use that map to predict how the cell behaves under different conditions (like different nutrients or gene changes). A common approach is constraint-based modeling (for example, Flux Balance Analysis). It treats the network as a set of pipes with limits and finds a feasible set of reaction rates that balance mass and optimize a goal (e.g., growth or production of a substance). Other methods use math to simulate time courses with kinetics.\n\nApplications include predicting growth, identifying gene knockouts that reduce disease or boost chemical production, and guiding metabolic engineering. Limitations come from incomplete data and simplifying assumptions. Analogy: reconstructing a city’s road map and running traffic simulations to see how congestion changes."}
{"Major": "Artificial Intelligence", "Term": "dynamic epistemic logic (DEL)", "Explanation": "Dynamic Epistemic Logic (DEL) is a formal way to study how people’s knowledge and beliefs change when events happen or information is shared. It combines epistemic logic (which models who knows what using possible worlds and each person’s indistinguishability between worlds) with dynamics (how actions like announcements, messages, or observations update that knowledge). \n\nIn DEL, an event (like a public announcement, a private message, or a statement someone makes) changes the knowledge state of the agents. After the event, agents may know new facts, realize that others know different things, or forget certain possibilities. A simple example: a public announcement “The coin is heads” rules out all worlds where the coin is tails, so everyone comes to know heads. More complex events handle private updates, simultaneous observations, or misdirection.\n\n DEL provides operators to say things like “after event a, agent i knows phi,” or “everyone will know phi after this communication,” and it handles nested knowledge (common knowledge: everyone knows that everyone knows, etc.). It’s used to verify multi‑agent protocols, security, game theory, and AI systems where agents reason about each other's knowledge and rationality."}
{"Major": "Artificial Intelligence", "Term": "neural Turing machine (NTM)", "Explanation": "Neural Turing Machine (NTM) is a type of AI model that combines a neural network with an external memory like a small computer inside the network. It has two main parts: a controller (usually a small neural net, like an LSTM) and a memory matrix that stores data. At each step, the controller decides what to read from memory and what to write back. It uses read and write heads that look at memory locations with a soft attention mechanism, so the actions are differentiable and can be trained with gradient descent.\n\nRead operations pull a read vector from memory, while write operations erase and then add information to chosen memory locations. Because everything is differentiable, the whole system can be trained end-to-end to learn simple algorithms that need memory, such as copying, sorting, or searching through data.\n\nNTMs are a bridge between neural networks and traditional computers: they show how a neural model can learn to control and manipulate an external memory like a program. Later models (e.g., the Differentiable Neural Computer) extended these ideas."}
{"Major": "Artificial Intelligence", "Term": "answer set programming (ASP)", "Explanation": "Answer set programming (ASP) is a way to solve tricky problems by writing down rules that describe how things could be true. You encode a problem as a small set of logical statements and constraints. An ASP solver then searches for all collections of facts—called answer sets—that satisfy every rule. Each answer set is one valid solution to the problem. There can be many solutions, or none at all, depending on the rules you give.\n\nKey ideas in plain terms:\n- Rules say what must be true if certain conditions hold (for example, if it’s Friday and it’s not raining, you go to the park).\n- Negation as failure lets you express defaults or exceptions (you assume something is true unless you have evidence it isn’t).\n- You can add constraints to rule out unwanted solutions, or use optimization to pick the best one.\n\nCommon uses include planning (figuring out a sequence of actions), scheduling (allocating time and resources), configuration (choosing a valid setup), and solving puzzles. ASP lets you model hard problems declaratively, and the solver does the heavy lifting of finding consistent, acceptable solutions."}
{"Major": "Artificial Intelligence", "Term": "NP-completeness", "Explanation": "NP-complete is a way to label some of the hardest problems in computer science.\n\n- NP means: if someone gives you a candidate solution, you can check whether it’s correct quickly (in time that grows polynomially with the input size).\n- A problem is NP-complete if two things hold: (1) it’s in NP, and (2) every problem in NP can be transformed into it quickly (in polynomial time). That second part is called a polynomial-time reduction.\n\nIntuition: NP-complete problems are the toughest problems in NP. If you had a fast, polynomial‑time algorithm for one NP-complete problem, you’d get fast algorithms for all NP problems. So they’re used as a benchmark for “hard” problems.\n\nExamples (decision versions): SAT (is there an assignment of true/false to variables making a formula true?), the decision version of traveling salesman, and clique (is there a clique of size k in a graph?).\n\nImportant caveat: we don’t know whether such fast algorithms exist for any NP-complete problem. This is the famous P vs NP question. Most researchers believe P ≠ NP, so these problems are likely intractable for large inputs, which is why heuristics and approximations are common."}
{"Major": "Artificial Intelligence", "Term": "partially observable Markov decision process (POMDP)", "Explanation": "Partially observable Markov decision process (POMDP) is a framework for planning when you can’t see the world perfectly.\n\nIn a POMDP, the world has many states, but you don’t know exactly which one is true. You take actions, the world changes in a probabilistic way, and you receive noisy observations that give clues about the true state. Your goal is to maximize total reward over time.\n\nKey pieces:\n- States S, actions A, observations O\n- Transition model T(s'|s,a): what state you might end up in after action a\n- Observation model Z(o|s',a): how likely you are to observe o in the new state\n- Reward R(s,a): how good an action is in a state\n\nBecause you don’t know the exact state, you maintain a belief b, a probability distribution over states. After acting and seeing an observation, you update b (Bayes’ rule). A policy tells you which action to take given your belief. Solving a POMDP exactly is computationally hard, so people use approximate methods; it’s common in robotics and planning under uncertainty."}
{"Major": "Artificial Intelligence", "Term": "quantum computing", "Explanation": "Quantum computing is a new way of computing that uses quantum bits, or qubits, instead of ordinary bits. A regular bit is 0 or 1. A qubit, however, can be 0, 1, or both at once (this is called superposition). When multiple qubits interact, they can become entangled, meaning the state of one qubit can depend on another, even when far apart.\n\nBecause of superposition and entanglement, a quantum computer can explore many possible answers at once and use interference to boost the chances of good answers while canceling out bad ones. For certain problems—like factoring large numbers, searching unsorted data, or simulating how molecules behave—this can be much faster than traditional computers.\n\nQuantum computers today aren’t better at all tasks and aren’t yet ready to replace regular computers. They’re noisy and prone to errors, and researchers are still building larger, more reliable machines (the NISQ era). In the future, they could help with cryptography, chemistry, optimization, and complex simulations, but only when the technology matures."}
